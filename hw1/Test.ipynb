{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_probs = np.load('distribution.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.choice(200 * 200, p=true_probs.reshape(-1), size=100000)\n",
    "samples_x = samples % 200\n",
    "samples_y = samples // 200\n",
    "samples = np.stack([samples_x, samples_y], axis=1)\n",
    "train, val = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "thetas = tf.Variable(np.zeros(200), dtype=tf.float32, name='thetas', trainable=True)\n",
    "ps = tf.exp(thetas) / tf.reduce_sum(tf.exp(thetas))\n",
    "\n",
    "input_data = tf.placeholder(dtype=tf.int32, shape=(None, 2))\n",
    "training = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "x1s = input_data[:, 0]\n",
    "x2s = input_data[:, 1]\n",
    "\n",
    "output = tf.layers.dense(inputs=tf.one_hot(x1s, depth=200), \n",
    "                         units=200, \n",
    "                         activation='relu', \n",
    "                         use_bias=True,\n",
    "                         kernel_initializer=tf.initializers.glorot_normal(),\n",
    "                         bias_initializer=tf.initializers.glorot_normal())\n",
    "\n",
    "output = tf.layers.dropout(inputs=output, training=training, rate=0.7)\n",
    "\n",
    "output = tf.layers.dense(inputs=output, units=200, activation='softmax',\n",
    "                         use_bias=True,\n",
    "                         kernel_initializer=tf.initializers.glorot_normal(),\n",
    "                         bias_initializer=tf.initializers.glorot_normal())\n",
    "\n",
    "p_x1 = tf.gather(ps, x1s)\n",
    "p_x2_x1 = tf.gather_nd(output, tf.stack([tf.range(tf.shape(input_data)[0]), x2s], axis=1))\n",
    "final_prob = p_x1 * p_x2_x1\n",
    "\n",
    "loss = tf.reduce_mean(-tf.log(p_x1) - tf.log(p_x2_x1))\n",
    "loss *= np.log2(np.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_step = opt.minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(10,)\n",
      "\n",
      "Step 0\ttrain_loss: 15.2875394821167\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 1\ttrain_loss: 15.315890312194824\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 2\ttrain_loss: 15.282522201538086\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 3\ttrain_loss: 15.25860595703125\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 4\ttrain_loss: 15.340636253356934\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 5\ttrain_loss: 15.331151962280273\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 6\ttrain_loss: 15.306975364685059\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 7\ttrain_loss: 15.334443092346191\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 8\ttrain_loss: 15.376666069030762\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 9\ttrain_loss: 15.337055206298828\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 10\ttrain_loss: 15.374058723449707\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 11\ttrain_loss: 15.267077445983887\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 12\ttrain_loss: 15.372364044189453\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 13\ttrain_loss: 15.33327579498291\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 14\ttrain_loss: 15.383152961730957\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 15\ttrain_loss: 15.285837173461914\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 16\ttrain_loss: 15.371665000915527\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 17\ttrain_loss: 15.282176971435547\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 18\ttrain_loss: 15.174394607543945\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 19\ttrain_loss: 15.296348571777344\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 20\ttrain_loss: 15.334239959716797\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 21\ttrain_loss: 15.132203102111816\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 22\ttrain_loss: 15.309324264526367\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 23\ttrain_loss: 15.250895500183105\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 24\ttrain_loss: 15.344107627868652\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 25\ttrain_loss: 15.244071006774902\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 26\ttrain_loss: 15.305286407470703\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 27\ttrain_loss: 15.247082710266113\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 28\ttrain_loss: 15.335993766784668\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 29\ttrain_loss: 15.378602027893066\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 30\ttrain_loss: 15.293184280395508\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 31\ttrain_loss: 15.224452018737793\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 32\ttrain_loss: 15.326567649841309\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 33\ttrain_loss: 15.26666259765625\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 34\ttrain_loss: 15.174975395202637\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 35\ttrain_loss: 15.31905460357666\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 36\ttrain_loss: 15.266782760620117\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 37\ttrain_loss: 15.448956489562988\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 38\ttrain_loss: 15.299285888671875\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 39\ttrain_loss: 15.251517295837402\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 40\ttrain_loss: 15.28073501586914\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 41\ttrain_loss: 15.201931953430176\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 42\ttrain_loss: 15.238409996032715\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 43\ttrain_loss: 15.340841293334961\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 44\ttrain_loss: 15.252073287963867\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 45\ttrain_loss: 15.39981460571289\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 46\ttrain_loss: 15.232117652893066\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 47\ttrain_loss: 15.062853813171387\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 48\ttrain_loss: 15.228822708129883\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 49\ttrain_loss: 15.315807342529297\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 50\ttrain_loss: 15.344039916992188\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 51\ttrain_loss: 15.234230995178223\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 52\ttrain_loss: 15.236136436462402\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 53\ttrain_loss: 15.302377700805664\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 54\ttrain_loss: 15.337410926818848\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 55\ttrain_loss: 15.232725143432617\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 56\ttrain_loss: 15.314266204833984\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 57\ttrain_loss: 15.44975757598877\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 58\ttrain_loss: 15.32882022857666\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 59\ttrain_loss: 15.218079566955566\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 60\ttrain_loss: 15.359009742736816\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 61\ttrain_loss: 15.24980354309082\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 62\ttrain_loss: 15.17725658416748\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 63\ttrain_loss: 15.482155799865723\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 64\ttrain_loss: 15.447806358337402\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 65\ttrain_loss: 15.326960563659668\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 66\ttrain_loss: 15.233758926391602\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 67\ttrain_loss: 15.255661964416504\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 68\ttrain_loss: 15.49953842163086\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 69\ttrain_loss: 15.253744125366211\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 70\ttrain_loss: 15.22478199005127\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 71\ttrain_loss: 15.297768592834473\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 72\ttrain_loss: 15.488659858703613\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 73\ttrain_loss: 15.262602806091309\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 74\ttrain_loss: 15.253113746643066\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 75\ttrain_loss: 15.279648780822754\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 76\ttrain_loss: 15.390494346618652\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 77\ttrain_loss: 15.22604751586914\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 78\ttrain_loss: 15.172003746032715\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 79\ttrain_loss: 15.303092002868652\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 80\ttrain_loss: 15.190035820007324\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 81\ttrain_loss: 15.289637565612793\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 82\ttrain_loss: 15.286434173583984\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 83\ttrain_loss: 15.175080299377441\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 84\ttrain_loss: 15.223433494567871\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 85\ttrain_loss: 15.284041404724121\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 86\ttrain_loss: 15.494332313537598\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 87\ttrain_loss: 15.315481185913086\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 88\ttrain_loss: 15.269111633300781\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 89\ttrain_loss: 15.313373565673828\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 90\ttrain_loss: 15.385907173156738\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 91\ttrain_loss: 15.132942199707031\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 92\ttrain_loss: 15.337721824645996\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 93\ttrain_loss: 15.403515815734863\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 94\ttrain_loss: 15.240392684936523\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 95\ttrain_loss: 15.325918197631836\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 96\ttrain_loss: 15.321898460388184\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 97\ttrain_loss: 15.495226860046387\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 98\ttrain_loss: 15.261938095092773\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 99\ttrain_loss: 15.307014465332031\tval_loss: 15.29515266418457\n",
      "(10,)\n",
      "\n",
      "Step 100\ttrain_loss: 15.353870391845703\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 101\ttrain_loss: 15.102192878723145\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 102\ttrain_loss: 15.257126808166504\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 103\ttrain_loss: 15.271671295166016\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 104\ttrain_loss: 15.236113548278809\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 105\ttrain_loss: 15.293691635131836\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 106\ttrain_loss: 15.335517883300781\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 107\ttrain_loss: 15.213208198547363\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 108\ttrain_loss: 15.210358619689941\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 109\ttrain_loss: 15.482274055480957\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 110\ttrain_loss: 15.366619110107422\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 111\ttrain_loss: 15.347359657287598\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 112\ttrain_loss: 15.367998123168945\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 113\ttrain_loss: 15.464832305908203\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 114\ttrain_loss: 15.317360877990723\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 115\ttrain_loss: 15.278172492980957\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 116\ttrain_loss: 15.234013557434082\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 117\ttrain_loss: 15.251016616821289\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 118\ttrain_loss: 15.30710220336914\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 119\ttrain_loss: 15.201432228088379\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 120\ttrain_loss: 15.391321182250977\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 121\ttrain_loss: 15.20626449584961\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 122\ttrain_loss: 15.237130165100098\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 123\ttrain_loss: 15.27417278289795\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 124\ttrain_loss: 15.094923973083496\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 125\ttrain_loss: 15.27828598022461\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 126\ttrain_loss: 15.367414474487305\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 127\ttrain_loss: 15.23486042022705\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 128\ttrain_loss: 15.336904525756836\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 129\ttrain_loss: 15.270272254943848\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 130\ttrain_loss: 15.233946800231934\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 131\ttrain_loss: 15.261837005615234\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 132\ttrain_loss: 15.23238754272461\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 133\ttrain_loss: 15.375836372375488\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 134\ttrain_loss: 15.307232856750488\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 135\ttrain_loss: 15.428216934204102\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 136\ttrain_loss: 15.28420352935791\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 137\ttrain_loss: 15.48112678527832\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 138\ttrain_loss: 15.392609596252441\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 139\ttrain_loss: 15.223983764648438\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 140\ttrain_loss: 15.23941421508789\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 141\ttrain_loss: 15.228866577148438\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 142\ttrain_loss: 15.351943969726562\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 143\ttrain_loss: 15.155680656433105\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 144\ttrain_loss: 15.466164588928223\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 145\ttrain_loss: 15.352532386779785\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 146\ttrain_loss: 15.16112995147705\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 147\ttrain_loss: 15.305766105651855\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 148\ttrain_loss: 15.26709270477295\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 149\ttrain_loss: 15.32579231262207\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 150\ttrain_loss: 15.445837020874023\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 151\ttrain_loss: 15.25731372833252\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 152\ttrain_loss: 15.360657691955566\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 153\ttrain_loss: 15.381051063537598\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 154\ttrain_loss: 15.24720287322998\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 155\ttrain_loss: 15.079626083374023\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 156\ttrain_loss: 15.429585456848145\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 157\ttrain_loss: 15.200897216796875\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 158\ttrain_loss: 15.154714584350586\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 159\ttrain_loss: 15.326601028442383\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 160\ttrain_loss: 15.349286079406738\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 161\ttrain_loss: 15.304353713989258\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 162\ttrain_loss: 15.154932022094727\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 163\ttrain_loss: 15.084917068481445\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 164\ttrain_loss: 15.239812850952148\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 165\ttrain_loss: 15.31559944152832\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 166\ttrain_loss: 15.284399032592773\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 167\ttrain_loss: 15.368438720703125\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 168\ttrain_loss: 15.384337425231934\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 169\ttrain_loss: 15.37080192565918\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 170\ttrain_loss: 15.135588645935059\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 171\ttrain_loss: 15.500896453857422\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 172\ttrain_loss: 15.14212703704834\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 173\ttrain_loss: 15.103007316589355\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 174\ttrain_loss: 15.287052154541016\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 175\ttrain_loss: 15.428472518920898\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 176\ttrain_loss: 15.264284133911133\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 177\ttrain_loss: 15.25074577331543\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 178\ttrain_loss: 15.28415584564209\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 179\ttrain_loss: 15.201085090637207\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 180\ttrain_loss: 15.181687355041504\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 181\ttrain_loss: 15.449332237243652\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 182\ttrain_loss: 15.334107398986816\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 183\ttrain_loss: 15.406062126159668\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 184\ttrain_loss: 15.220417976379395\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 185\ttrain_loss: 15.207385063171387\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 186\ttrain_loss: 15.144233703613281\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 187\ttrain_loss: 15.248969078063965\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 188\ttrain_loss: 15.340775489807129\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 189\ttrain_loss: 15.519356727600098\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 190\ttrain_loss: 15.285533905029297\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 191\ttrain_loss: 15.094562530517578\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 192\ttrain_loss: 15.392843246459961\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 193\ttrain_loss: 15.067195892333984\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 194\ttrain_loss: 15.438166618347168\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 195\ttrain_loss: 15.316038131713867\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 196\ttrain_loss: 15.314261436462402\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 197\ttrain_loss: 15.228925704956055\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 198\ttrain_loss: 15.261701583862305\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n",
      "Step 199\ttrain_loss: 15.338911056518555\tval_loss: 15.289188385009766\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 200\ttrain_loss: 15.34085464477539\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 201\ttrain_loss: 15.268233299255371\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 202\ttrain_loss: 15.269304275512695\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 203\ttrain_loss: 15.361624717712402\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 204\ttrain_loss: 15.361021995544434\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 205\ttrain_loss: 15.274092674255371\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 206\ttrain_loss: 15.245654106140137\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 207\ttrain_loss: 15.33840560913086\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 208\ttrain_loss: 15.19796085357666\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 209\ttrain_loss: 15.159292221069336\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 210\ttrain_loss: 15.475557327270508\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 211\ttrain_loss: 15.318209648132324\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 212\ttrain_loss: 15.501691818237305\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 213\ttrain_loss: 15.318860054016113\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 214\ttrain_loss: 15.250938415527344\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 215\ttrain_loss: 15.30035400390625\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 216\ttrain_loss: 15.425858497619629\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 217\ttrain_loss: 15.286508560180664\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 218\ttrain_loss: 15.197220802307129\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 219\ttrain_loss: 15.270976066589355\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 220\ttrain_loss: 15.266206741333008\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 221\ttrain_loss: 15.448073387145996\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 222\ttrain_loss: 15.257637023925781\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 223\ttrain_loss: 15.222761154174805\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 224\ttrain_loss: 15.433643341064453\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 225\ttrain_loss: 15.148438453674316\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 226\ttrain_loss: 15.415690422058105\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 227\ttrain_loss: 15.421504974365234\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 228\ttrain_loss: 15.124984741210938\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 229\ttrain_loss: 15.179618835449219\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 230\ttrain_loss: 15.242053031921387\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 231\ttrain_loss: 15.53271484375\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 232\ttrain_loss: 15.112427711486816\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 233\ttrain_loss: 15.298465728759766\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 234\ttrain_loss: 15.63884449005127\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 235\ttrain_loss: 15.365774154663086\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 236\ttrain_loss: 15.198695182800293\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 237\ttrain_loss: 15.277359962463379\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 238\ttrain_loss: 15.078312873840332\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 239\ttrain_loss: 15.281813621520996\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 240\ttrain_loss: 15.318082809448242\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 241\ttrain_loss: 15.19635009765625\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 242\ttrain_loss: 15.32528018951416\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 243\ttrain_loss: 15.51912784576416\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 244\ttrain_loss: 15.129745483398438\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 245\ttrain_loss: 15.413247108459473\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 246\ttrain_loss: 15.313183784484863\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 247\ttrain_loss: 15.257599830627441\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 248\ttrain_loss: 15.325689315795898\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 249\ttrain_loss: 15.175498962402344\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 250\ttrain_loss: 15.09914779663086\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 251\ttrain_loss: 15.34203815460205\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 252\ttrain_loss: 15.49367618560791\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 253\ttrain_loss: 15.171956062316895\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 254\ttrain_loss: 15.281047821044922\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 255\ttrain_loss: 15.435101509094238\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 256\ttrain_loss: 15.40261459350586\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 257\ttrain_loss: 15.396775245666504\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 258\ttrain_loss: 15.455282211303711\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 259\ttrain_loss: 15.278776168823242\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 260\ttrain_loss: 15.203775405883789\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 261\ttrain_loss: 15.320266723632812\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 262\ttrain_loss: 15.250492095947266\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 263\ttrain_loss: 15.43671989440918\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 264\ttrain_loss: 15.465929985046387\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 265\ttrain_loss: 15.277131080627441\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 266\ttrain_loss: 15.35384750366211\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 267\ttrain_loss: 15.397807121276855\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 268\ttrain_loss: 15.189659118652344\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 269\ttrain_loss: 15.329422950744629\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 270\ttrain_loss: 15.337894439697266\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 271\ttrain_loss: 15.320449829101562\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 272\ttrain_loss: 15.49722671508789\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 273\ttrain_loss: 15.522061347961426\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 274\ttrain_loss: 15.335655212402344\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 275\ttrain_loss: 15.207819938659668\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 276\ttrain_loss: 15.138749122619629\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 277\ttrain_loss: 15.478385925292969\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 278\ttrain_loss: 15.273355484008789\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 279\ttrain_loss: 15.337789535522461\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 280\ttrain_loss: 15.247949600219727\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 281\ttrain_loss: 15.33536434173584\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 282\ttrain_loss: 15.253680229187012\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 283\ttrain_loss: 15.150129318237305\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 284\ttrain_loss: 15.34138298034668\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 285\ttrain_loss: 15.34807300567627\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 286\ttrain_loss: 15.271696090698242\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 287\ttrain_loss: 15.35233211517334\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 288\ttrain_loss: 15.273085594177246\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 289\ttrain_loss: 15.373589515686035\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 290\ttrain_loss: 15.207856178283691\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 291\ttrain_loss: 15.242395401000977\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 292\ttrain_loss: 15.783564567565918\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 293\ttrain_loss: 15.346254348754883\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 294\ttrain_loss: 15.339861869812012\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 295\ttrain_loss: 15.447488784790039\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 296\ttrain_loss: 15.352825164794922\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 297\ttrain_loss: 15.249838829040527\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 298\ttrain_loss: 15.243069648742676\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 299\ttrain_loss: 15.19222354888916\tval_loss: 15.287711143493652\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 300\ttrain_loss: 15.404227256774902\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 301\ttrain_loss: 15.313821792602539\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 302\ttrain_loss: 15.309507369995117\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 303\ttrain_loss: 15.225913047790527\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 304\ttrain_loss: 15.263404846191406\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 305\ttrain_loss: 15.424155235290527\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 306\ttrain_loss: 15.481502532958984\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 307\ttrain_loss: 15.450135231018066\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 308\ttrain_loss: 15.432764053344727\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 309\ttrain_loss: 15.467924118041992\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 310\ttrain_loss: 15.423842430114746\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 311\ttrain_loss: 15.27509880065918\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 312\ttrain_loss: 15.420260429382324\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 313\ttrain_loss: 15.189939498901367\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 314\ttrain_loss: 15.499063491821289\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 315\ttrain_loss: 15.463943481445312\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 316\ttrain_loss: 15.299668312072754\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 317\ttrain_loss: 15.349132537841797\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 318\ttrain_loss: 15.535372734069824\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 319\ttrain_loss: 15.316075325012207\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 320\ttrain_loss: 15.444366455078125\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 321\ttrain_loss: 15.245835304260254\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 322\ttrain_loss: 15.288848876953125\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 323\ttrain_loss: 15.138578414916992\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 324\ttrain_loss: 15.36890983581543\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 325\ttrain_loss: 15.3430757522583\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 326\ttrain_loss: 15.28709888458252\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 327\ttrain_loss: 15.170686721801758\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 328\ttrain_loss: 15.362317085266113\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 329\ttrain_loss: 15.37021541595459\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 330\ttrain_loss: 15.223750114440918\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 331\ttrain_loss: 15.565459251403809\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 332\ttrain_loss: 15.201874732971191\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 333\ttrain_loss: 15.26256275177002\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 334\ttrain_loss: 15.294300079345703\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 335\ttrain_loss: 15.519227027893066\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 336\ttrain_loss: 15.159473419189453\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 337\ttrain_loss: 15.259730339050293\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 338\ttrain_loss: 15.390881538391113\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 339\ttrain_loss: 15.156830787658691\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 340\ttrain_loss: 15.377141952514648\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 341\ttrain_loss: 15.236039161682129\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 342\ttrain_loss: 15.273866653442383\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 343\ttrain_loss: 15.298141479492188\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 344\ttrain_loss: 15.2433500289917\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 345\ttrain_loss: 15.26515007019043\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 346\ttrain_loss: 15.262648582458496\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 347\ttrain_loss: 15.231549263000488\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 348\ttrain_loss: 15.218992233276367\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 349\ttrain_loss: 15.299151420593262\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 350\ttrain_loss: 15.2923583984375\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 351\ttrain_loss: 15.336705207824707\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 352\ttrain_loss: 15.247302055358887\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 353\ttrain_loss: 15.240047454833984\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 354\ttrain_loss: 15.214224815368652\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 355\ttrain_loss: 15.373517036437988\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 356\ttrain_loss: 15.209879875183105\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 357\ttrain_loss: 15.28079605102539\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 358\ttrain_loss: 15.214174270629883\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 359\ttrain_loss: 15.31730842590332\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 360\ttrain_loss: 15.432964324951172\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 361\ttrain_loss: 15.209735870361328\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 362\ttrain_loss: 15.471405982971191\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 363\ttrain_loss: 15.166397094726562\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 364\ttrain_loss: 15.237878799438477\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 365\ttrain_loss: 15.199843406677246\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 366\ttrain_loss: 15.347996711730957\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 367\ttrain_loss: 15.409741401672363\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 368\ttrain_loss: 15.518009185791016\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 369\ttrain_loss: 15.315080642700195\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 370\ttrain_loss: 15.232921600341797\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 371\ttrain_loss: 15.307147979736328\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 372\ttrain_loss: 15.072295188903809\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 373\ttrain_loss: 15.485881805419922\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 374\ttrain_loss: 15.107057571411133\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 375\ttrain_loss: 15.347613334655762\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 376\ttrain_loss: 15.322575569152832\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 377\ttrain_loss: 15.284006118774414\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 378\ttrain_loss: 15.395508766174316\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 379\ttrain_loss: 15.317124366760254\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 380\ttrain_loss: 15.34990119934082\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 381\ttrain_loss: 15.274089813232422\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 382\ttrain_loss: 15.27469253540039\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 383\ttrain_loss: 15.343791007995605\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 384\ttrain_loss: 15.30374526977539\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 385\ttrain_loss: 15.277970314025879\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 386\ttrain_loss: 15.322616577148438\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 387\ttrain_loss: 15.089882850646973\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 388\ttrain_loss: 15.062902450561523\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 389\ttrain_loss: 15.230307579040527\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 390\ttrain_loss: 15.402796745300293\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 391\ttrain_loss: 15.105052947998047\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 392\ttrain_loss: 15.195119857788086\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 393\ttrain_loss: 15.256396293640137\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 394\ttrain_loss: 15.221680641174316\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 395\ttrain_loss: 15.516183853149414\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 396\ttrain_loss: 15.160453796386719\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 397\ttrain_loss: 15.382037162780762\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 398\ttrain_loss: 15.303808212280273\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 399\ttrain_loss: 15.303285598754883\tval_loss: 15.282882690429688\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 400\ttrain_loss: 15.369791984558105\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 401\ttrain_loss: 15.505545616149902\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 402\ttrain_loss: 15.41698932647705\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 403\ttrain_loss: 15.432096481323242\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 404\ttrain_loss: 15.384593963623047\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 405\ttrain_loss: 15.302863121032715\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 406\ttrain_loss: 15.231781959533691\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 407\ttrain_loss: 15.182122230529785\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 408\ttrain_loss: 15.209575653076172\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 409\ttrain_loss: 15.477518081665039\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 410\ttrain_loss: 15.140606880187988\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 411\ttrain_loss: 15.133337020874023\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 412\ttrain_loss: 15.239949226379395\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 413\ttrain_loss: 15.129444122314453\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 414\ttrain_loss: 15.431478500366211\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 415\ttrain_loss: 15.465432167053223\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 416\ttrain_loss: 15.181600570678711\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 417\ttrain_loss: 15.2251558303833\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 418\ttrain_loss: 15.309006690979004\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 419\ttrain_loss: 15.127296447753906\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 420\ttrain_loss: 15.347495079040527\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 421\ttrain_loss: 15.14457893371582\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 422\ttrain_loss: 15.350838661193848\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 423\ttrain_loss: 15.513803482055664\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 424\ttrain_loss: 15.313767433166504\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 425\ttrain_loss: 15.295646667480469\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 426\ttrain_loss: 15.318026542663574\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 427\ttrain_loss: 15.257664680480957\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 428\ttrain_loss: 15.396265029907227\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 429\ttrain_loss: 15.348088264465332\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 430\ttrain_loss: 15.324543952941895\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 431\ttrain_loss: 15.399931907653809\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 432\ttrain_loss: 15.21083927154541\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 433\ttrain_loss: 15.227673530578613\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 434\ttrain_loss: 15.165571212768555\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 435\ttrain_loss: 15.331583023071289\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 436\ttrain_loss: 15.13021469116211\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 437\ttrain_loss: 15.324493408203125\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 438\ttrain_loss: 15.327751159667969\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 439\ttrain_loss: 15.287749290466309\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 440\ttrain_loss: 15.20678424835205\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 441\ttrain_loss: 15.271425247192383\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 442\ttrain_loss: 15.322587966918945\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 443\ttrain_loss: 15.20963191986084\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 444\ttrain_loss: 15.27868366241455\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 445\ttrain_loss: 15.257257461547852\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 446\ttrain_loss: 15.272481918334961\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 447\ttrain_loss: 15.263655662536621\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 448\ttrain_loss: 15.214611053466797\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 449\ttrain_loss: 15.372392654418945\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 450\ttrain_loss: 15.397188186645508\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 451\ttrain_loss: 15.33675765991211\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 452\ttrain_loss: 15.433695793151855\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 453\ttrain_loss: 15.35068130493164\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 454\ttrain_loss: 15.290977478027344\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 455\ttrain_loss: 15.344728469848633\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 456\ttrain_loss: 15.298362731933594\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 457\ttrain_loss: 15.223159790039062\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 458\ttrain_loss: 15.20294189453125\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 459\ttrain_loss: 15.160408020019531\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 460\ttrain_loss: 15.283228874206543\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 461\ttrain_loss: 15.248644828796387\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 462\ttrain_loss: 15.22728443145752\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 463\ttrain_loss: 15.14082145690918\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 464\ttrain_loss: 15.188387870788574\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 465\ttrain_loss: 15.209037780761719\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 466\ttrain_loss: 15.15005874633789\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 467\ttrain_loss: 15.248457908630371\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 468\ttrain_loss: 15.304736137390137\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 469\ttrain_loss: 15.358370780944824\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 470\ttrain_loss: 15.193781852722168\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 471\ttrain_loss: 15.355422973632812\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 472\ttrain_loss: 15.357069969177246\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 473\ttrain_loss: 15.205613136291504\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 474\ttrain_loss: 15.157645225524902\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 475\ttrain_loss: 15.548256874084473\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 476\ttrain_loss: 15.229255676269531\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 477\ttrain_loss: 15.249988555908203\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 478\ttrain_loss: 15.227527618408203\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 479\ttrain_loss: 15.198322296142578\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 480\ttrain_loss: 15.216551780700684\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 481\ttrain_loss: 15.215750694274902\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 482\ttrain_loss: 15.35742473602295\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 483\ttrain_loss: 15.350767135620117\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 484\ttrain_loss: 15.123600959777832\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 485\ttrain_loss: 15.360331535339355\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 486\ttrain_loss: 15.374897003173828\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 487\ttrain_loss: 15.355029106140137\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 488\ttrain_loss: 15.275361061096191\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 489\ttrain_loss: 15.158236503601074\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 490\ttrain_loss: 15.271951675415039\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 491\ttrain_loss: 15.412639617919922\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 492\ttrain_loss: 15.14232063293457\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 493\ttrain_loss: 15.249844551086426\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 494\ttrain_loss: 15.365320205688477\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 495\ttrain_loss: 15.388057708740234\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 496\ttrain_loss: 15.36933422088623\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 497\ttrain_loss: 15.342256546020508\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 498\ttrain_loss: 15.275720596313477\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 499\ttrain_loss: 15.19992733001709\tval_loss: 15.27816104888916\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 500\ttrain_loss: 15.298321723937988\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 501\ttrain_loss: 15.442012786865234\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 502\ttrain_loss: 15.314900398254395\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 503\ttrain_loss: 15.14015007019043\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 504\ttrain_loss: 15.25770092010498\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 505\ttrain_loss: 15.196126937866211\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 506\ttrain_loss: 15.397047996520996\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 507\ttrain_loss: 15.222591400146484\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 508\ttrain_loss: 15.134106636047363\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 509\ttrain_loss: 15.32236385345459\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 510\ttrain_loss: 15.252300262451172\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 511\ttrain_loss: 15.239849090576172\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 512\ttrain_loss: 15.38010311126709\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 513\ttrain_loss: 15.358748435974121\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 514\ttrain_loss: 15.308320999145508\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 515\ttrain_loss: 15.14356517791748\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 516\ttrain_loss: 15.101543426513672\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 517\ttrain_loss: 15.34517765045166\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 518\ttrain_loss: 15.447555541992188\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 519\ttrain_loss: 15.271988868713379\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 520\ttrain_loss: 15.13732624053955\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 521\ttrain_loss: 15.311294555664062\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 522\ttrain_loss: 15.239486694335938\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 523\ttrain_loss: 15.236262321472168\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 524\ttrain_loss: 15.44871997833252\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 525\ttrain_loss: 15.187801361083984\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 526\ttrain_loss: 15.103816032409668\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 527\ttrain_loss: 15.451087951660156\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 528\ttrain_loss: 15.256429672241211\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 529\ttrain_loss: 15.30301570892334\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 530\ttrain_loss: 15.369709968566895\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 531\ttrain_loss: 15.21842098236084\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 532\ttrain_loss: 15.221939086914062\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 533\ttrain_loss: 15.547882080078125\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 534\ttrain_loss: 15.248405456542969\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 535\ttrain_loss: 15.343056678771973\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 536\ttrain_loss: 15.211067199707031\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 537\ttrain_loss: 15.24970817565918\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 538\ttrain_loss: 15.215630531311035\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 539\ttrain_loss: 15.129788398742676\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 540\ttrain_loss: 15.378592491149902\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 541\ttrain_loss: 15.443340301513672\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 542\ttrain_loss: 15.303388595581055\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 543\ttrain_loss: 15.509995460510254\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 544\ttrain_loss: 15.2510347366333\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 545\ttrain_loss: 15.324029922485352\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 546\ttrain_loss: 15.29407024383545\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 547\ttrain_loss: 15.33313274383545\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 548\ttrain_loss: 15.163490295410156\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 549\ttrain_loss: 15.271272659301758\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 550\ttrain_loss: 15.259392738342285\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 551\ttrain_loss: 15.04587459564209\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 552\ttrain_loss: 15.305059432983398\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 553\ttrain_loss: 15.534712791442871\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 554\ttrain_loss: 15.191930770874023\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 555\ttrain_loss: 15.243680953979492\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 556\ttrain_loss: 15.355213165283203\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 557\ttrain_loss: 15.190841674804688\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 558\ttrain_loss: 15.065171241760254\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 559\ttrain_loss: 15.337311744689941\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 560\ttrain_loss: 15.204025268554688\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 561\ttrain_loss: 15.343249320983887\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 562\ttrain_loss: 15.319361686706543\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 563\ttrain_loss: 15.450028419494629\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 564\ttrain_loss: 15.092387199401855\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 565\ttrain_loss: 15.176923751831055\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 566\ttrain_loss: 15.212652206420898\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 567\ttrain_loss: 15.120772361755371\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 568\ttrain_loss: 15.269613265991211\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 569\ttrain_loss: 15.399209022521973\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 570\ttrain_loss: 15.168292999267578\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 571\ttrain_loss: 15.464550018310547\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 572\ttrain_loss: 15.43664264678955\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 573\ttrain_loss: 15.204363822937012\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 574\ttrain_loss: 15.27824878692627\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 575\ttrain_loss: 15.122735977172852\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 576\ttrain_loss: 15.512354850769043\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 577\ttrain_loss: 15.354655265808105\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 578\ttrain_loss: 15.2432222366333\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 579\ttrain_loss: 15.365602493286133\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 580\ttrain_loss: 15.44999885559082\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 581\ttrain_loss: 15.253085136413574\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 582\ttrain_loss: 15.267373085021973\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 583\ttrain_loss: 15.284405708312988\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 584\ttrain_loss: 15.21755599975586\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 585\ttrain_loss: 15.367280960083008\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 586\ttrain_loss: 15.281803131103516\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 587\ttrain_loss: 15.347625732421875\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 588\ttrain_loss: 15.254241943359375\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 589\ttrain_loss: 15.201203346252441\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 590\ttrain_loss: 15.28123664855957\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 591\ttrain_loss: 15.277698516845703\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 592\ttrain_loss: 15.097869873046875\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 593\ttrain_loss: 15.260581970214844\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 594\ttrain_loss: 15.294319152832031\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 595\ttrain_loss: 15.371851921081543\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 596\ttrain_loss: 15.316210746765137\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 597\ttrain_loss: 15.164670944213867\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 598\ttrain_loss: 15.155413627624512\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 599\ttrain_loss: 15.137813568115234\tval_loss: 15.275039672851562\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 600\ttrain_loss: 15.41816234588623\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 601\ttrain_loss: 15.185074806213379\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 602\ttrain_loss: 15.255854606628418\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 603\ttrain_loss: 15.14868450164795\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 604\ttrain_loss: 15.211019515991211\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 605\ttrain_loss: 15.353434562683105\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 606\ttrain_loss: 15.099464416503906\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 607\ttrain_loss: 15.289993286132812\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 608\ttrain_loss: 15.32863712310791\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 609\ttrain_loss: 15.156167984008789\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 610\ttrain_loss: 15.108670234680176\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 611\ttrain_loss: 15.436942100524902\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 612\ttrain_loss: 15.249866485595703\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 613\ttrain_loss: 15.175919532775879\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 614\ttrain_loss: 15.22933292388916\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 615\ttrain_loss: 15.2673921585083\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 616\ttrain_loss: 15.185904502868652\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 617\ttrain_loss: 15.322616577148438\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 618\ttrain_loss: 15.128643989562988\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 619\ttrain_loss: 15.089858055114746\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 620\ttrain_loss: 15.218790054321289\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 621\ttrain_loss: 15.501776695251465\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 622\ttrain_loss: 15.553982734680176\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 623\ttrain_loss: 15.299051284790039\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 624\ttrain_loss: 15.234184265136719\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 625\ttrain_loss: 15.059638977050781\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 626\ttrain_loss: 15.295626640319824\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 627\ttrain_loss: 15.292712211608887\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 628\ttrain_loss: 15.413520812988281\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 629\ttrain_loss: 15.156000137329102\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 630\ttrain_loss: 15.319795608520508\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 631\ttrain_loss: 15.428544044494629\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 632\ttrain_loss: 15.130491256713867\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 633\ttrain_loss: 15.329683303833008\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 634\ttrain_loss: 15.538132667541504\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 635\ttrain_loss: 15.296327590942383\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 636\ttrain_loss: 15.181601524353027\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 637\ttrain_loss: 15.337213516235352\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 638\ttrain_loss: 15.51905632019043\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 639\ttrain_loss: 15.233102798461914\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 640\ttrain_loss: 15.372772216796875\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 641\ttrain_loss: 15.206612586975098\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 642\ttrain_loss: 15.269681930541992\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 643\ttrain_loss: 15.241886138916016\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 644\ttrain_loss: 15.359109878540039\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 645\ttrain_loss: 15.33775520324707\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 646\ttrain_loss: 15.248100280761719\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 647\ttrain_loss: 15.210495948791504\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 648\ttrain_loss: 15.295549392700195\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 649\ttrain_loss: 15.174639701843262\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 650\ttrain_loss: 15.241418838500977\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 651\ttrain_loss: 15.241776466369629\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 652\ttrain_loss: 15.120318412780762\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 653\ttrain_loss: 15.446937561035156\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 654\ttrain_loss: 15.118741035461426\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 655\ttrain_loss: 15.375778198242188\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 656\ttrain_loss: 15.425128936767578\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 657\ttrain_loss: 15.327850341796875\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 658\ttrain_loss: 15.293354988098145\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 659\ttrain_loss: 15.149648666381836\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 660\ttrain_loss: 15.201319694519043\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 661\ttrain_loss: 15.190918922424316\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 662\ttrain_loss: 15.427231788635254\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 663\ttrain_loss: 15.280640602111816\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 664\ttrain_loss: 15.23025894165039\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 665\ttrain_loss: 15.390295028686523\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 666\ttrain_loss: 15.316221237182617\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 667\ttrain_loss: 15.100751876831055\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 668\ttrain_loss: 15.143902778625488\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 669\ttrain_loss: 15.487510681152344\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 670\ttrain_loss: 15.287566184997559\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 671\ttrain_loss: 15.433243751525879\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 672\ttrain_loss: 15.184670448303223\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 673\ttrain_loss: 15.160944938659668\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 674\ttrain_loss: 15.462570190429688\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 675\ttrain_loss: 15.362516403198242\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 676\ttrain_loss: 15.108272552490234\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 677\ttrain_loss: 15.175970077514648\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 678\ttrain_loss: 15.335737228393555\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 679\ttrain_loss: 15.266096115112305\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 680\ttrain_loss: 15.2014799118042\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 681\ttrain_loss: 15.336834907531738\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 682\ttrain_loss: 15.216257095336914\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 683\ttrain_loss: 15.395767211914062\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 684\ttrain_loss: 15.284006118774414\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 685\ttrain_loss: 15.252359390258789\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 686\ttrain_loss: 15.148971557617188\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 687\ttrain_loss: 15.331932067871094\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 688\ttrain_loss: 15.488042831420898\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 689\ttrain_loss: 15.171183586120605\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 690\ttrain_loss: 15.411802291870117\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 691\ttrain_loss: 15.24996566772461\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 692\ttrain_loss: 15.205392837524414\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 693\ttrain_loss: 15.208250999450684\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 694\ttrain_loss: 15.397300720214844\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 695\ttrain_loss: 15.317998886108398\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 696\ttrain_loss: 15.313032150268555\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 697\ttrain_loss: 15.181231498718262\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 698\ttrain_loss: 15.24758243560791\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 699\ttrain_loss: 15.10722827911377\tval_loss: 15.271615982055664\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 700\ttrain_loss: 15.355194091796875\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 701\ttrain_loss: 15.35678768157959\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 702\ttrain_loss: 15.30062484741211\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 703\ttrain_loss: 15.325979232788086\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 704\ttrain_loss: 15.241997718811035\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 705\ttrain_loss: 15.177234649658203\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 706\ttrain_loss: 15.23733901977539\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 707\ttrain_loss: 15.146258354187012\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 708\ttrain_loss: 15.227272033691406\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 709\ttrain_loss: 15.376880645751953\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 710\ttrain_loss: 15.257960319519043\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 711\ttrain_loss: 15.166399955749512\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 712\ttrain_loss: 15.278582572937012\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 713\ttrain_loss: 15.364031791687012\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 714\ttrain_loss: 15.507521629333496\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 715\ttrain_loss: 15.166598320007324\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 716\ttrain_loss: 15.182051658630371\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 717\ttrain_loss: 15.329061508178711\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 718\ttrain_loss: 15.461811065673828\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 719\ttrain_loss: 15.350637435913086\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 720\ttrain_loss: 15.314834594726562\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 721\ttrain_loss: 15.339197158813477\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 722\ttrain_loss: 15.422142028808594\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 723\ttrain_loss: 15.392047882080078\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 724\ttrain_loss: 15.366673469543457\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 725\ttrain_loss: 15.13548469543457\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 726\ttrain_loss: 15.089969635009766\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 727\ttrain_loss: 15.26519775390625\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 728\ttrain_loss: 15.122416496276855\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 729\ttrain_loss: 15.28877067565918\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 730\ttrain_loss: 15.253416061401367\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 731\ttrain_loss: 15.133240699768066\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 732\ttrain_loss: 15.514264106750488\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 733\ttrain_loss: 15.24101734161377\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 734\ttrain_loss: 15.2442045211792\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 735\ttrain_loss: 15.1585054397583\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 736\ttrain_loss: 15.301976203918457\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 737\ttrain_loss: 15.19699478149414\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 738\ttrain_loss: 15.385603904724121\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 739\ttrain_loss: 15.319561004638672\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 740\ttrain_loss: 15.326309204101562\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 741\ttrain_loss: 15.232046127319336\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 742\ttrain_loss: 15.260358810424805\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 743\ttrain_loss: 15.086896896362305\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 744\ttrain_loss: 15.32136058807373\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 745\ttrain_loss: 15.241714477539062\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 746\ttrain_loss: 15.273113250732422\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 747\ttrain_loss: 15.282870292663574\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 748\ttrain_loss: 15.323841094970703\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 749\ttrain_loss: 15.299012184143066\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 750\ttrain_loss: 15.264451026916504\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 751\ttrain_loss: 15.219341278076172\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 752\ttrain_loss: 15.31538200378418\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 753\ttrain_loss: 15.429197311401367\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 754\ttrain_loss: 15.123323440551758\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 755\ttrain_loss: 15.148709297180176\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 756\ttrain_loss: 15.529037475585938\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 757\ttrain_loss: 15.174720764160156\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 758\ttrain_loss: 15.174527168273926\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 759\ttrain_loss: 15.081692695617676\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 760\ttrain_loss: 15.295319557189941\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 761\ttrain_loss: 15.325360298156738\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 762\ttrain_loss: 15.103405952453613\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 763\ttrain_loss: 15.255975723266602\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 764\ttrain_loss: 15.384564399719238\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 765\ttrain_loss: 15.173811912536621\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 766\ttrain_loss: 15.293171882629395\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 767\ttrain_loss: 15.289689064025879\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 768\ttrain_loss: 15.362967491149902\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 769\ttrain_loss: 15.47562026977539\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 770\ttrain_loss: 15.086479187011719\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 771\ttrain_loss: 15.271580696105957\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 772\ttrain_loss: 15.316741943359375\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 773\ttrain_loss: 15.345622062683105\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 774\ttrain_loss: 15.268853187561035\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 775\ttrain_loss: 15.196925163269043\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 776\ttrain_loss: 15.35421085357666\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 777\ttrain_loss: 15.499176979064941\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 778\ttrain_loss: 15.2682523727417\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 779\ttrain_loss: 15.304895401000977\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 780\ttrain_loss: 15.259599685668945\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 781\ttrain_loss: 15.192207336425781\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 782\ttrain_loss: 15.266695022583008\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 783\ttrain_loss: 15.37093448638916\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 784\ttrain_loss: 15.312920570373535\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 785\ttrain_loss: 15.299994468688965\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 786\ttrain_loss: 15.32466983795166\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 787\ttrain_loss: 15.364107131958008\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 788\ttrain_loss: 15.291446685791016\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 789\ttrain_loss: 15.312773704528809\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 790\ttrain_loss: 15.439470291137695\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 791\ttrain_loss: 15.280218124389648\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 792\ttrain_loss: 15.250964164733887\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 793\ttrain_loss: 15.333535194396973\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 794\ttrain_loss: 15.369054794311523\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 795\ttrain_loss: 15.379109382629395\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 796\ttrain_loss: 15.418248176574707\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 797\ttrain_loss: 15.281144142150879\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 798\ttrain_loss: 15.16895866394043\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 799\ttrain_loss: 15.227527618408203\tval_loss: 15.269146919250488\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 800\ttrain_loss: 15.456350326538086\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 801\ttrain_loss: 15.433938980102539\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 802\ttrain_loss: 15.524736404418945\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 803\ttrain_loss: 15.472947120666504\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 804\ttrain_loss: 15.318610191345215\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 805\ttrain_loss: 15.293859481811523\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 806\ttrain_loss: 15.310629844665527\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 807\ttrain_loss: 15.342693328857422\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 808\ttrain_loss: 15.470162391662598\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 809\ttrain_loss: 15.252513885498047\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 810\ttrain_loss: 15.282980918884277\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 811\ttrain_loss: 15.177815437316895\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 812\ttrain_loss: 15.214343070983887\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 813\ttrain_loss: 15.09731388092041\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 814\ttrain_loss: 15.284923553466797\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 815\ttrain_loss: 15.358609199523926\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 816\ttrain_loss: 15.367485046386719\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 817\ttrain_loss: 15.3521728515625\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 818\ttrain_loss: 15.161293983459473\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 819\ttrain_loss: 15.259488105773926\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 820\ttrain_loss: 15.211490631103516\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 821\ttrain_loss: 15.281700134277344\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 822\ttrain_loss: 15.3057861328125\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 823\ttrain_loss: 15.231149673461914\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 824\ttrain_loss: 15.49046516418457\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 825\ttrain_loss: 15.304533958435059\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 826\ttrain_loss: 15.449213981628418\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 827\ttrain_loss: 15.22254753112793\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 828\ttrain_loss: 15.090225219726562\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 829\ttrain_loss: 15.273956298828125\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 830\ttrain_loss: 15.377694129943848\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 831\ttrain_loss: 15.327250480651855\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 832\ttrain_loss: 15.266310691833496\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 833\ttrain_loss: 15.307039260864258\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 834\ttrain_loss: 15.389824867248535\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 835\ttrain_loss: 15.359793663024902\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 836\ttrain_loss: 15.244935035705566\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 837\ttrain_loss: 15.174570083618164\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 838\ttrain_loss: 15.320547103881836\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 839\ttrain_loss: 15.281830787658691\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 840\ttrain_loss: 15.314349174499512\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 841\ttrain_loss: 15.122291564941406\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 842\ttrain_loss: 15.375473976135254\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 843\ttrain_loss: 15.306921005249023\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 844\ttrain_loss: 15.346293449401855\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 845\ttrain_loss: 15.307769775390625\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 846\ttrain_loss: 15.176486015319824\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 847\ttrain_loss: 15.168249130249023\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 848\ttrain_loss: 15.206851959228516\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 849\ttrain_loss: 15.459586143493652\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 850\ttrain_loss: 15.296249389648438\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 851\ttrain_loss: 15.221182823181152\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 852\ttrain_loss: 15.076942443847656\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 853\ttrain_loss: 15.344676971435547\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 854\ttrain_loss: 15.309582710266113\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 855\ttrain_loss: 15.221903800964355\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 856\ttrain_loss: 15.19407844543457\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 857\ttrain_loss: 15.18619155883789\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 858\ttrain_loss: 15.421784400939941\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 859\ttrain_loss: 15.379240036010742\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 860\ttrain_loss: 15.131245613098145\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 861\ttrain_loss: 15.353570938110352\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 862\ttrain_loss: 15.323766708374023\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 863\ttrain_loss: 15.17232608795166\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 864\ttrain_loss: 15.231243133544922\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 865\ttrain_loss: 15.329864501953125\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 866\ttrain_loss: 15.192468643188477\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 867\ttrain_loss: 15.206846237182617\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 868\ttrain_loss: 15.352611541748047\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 869\ttrain_loss: 15.252486228942871\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 870\ttrain_loss: 15.204798698425293\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 871\ttrain_loss: 15.175602912902832\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 872\ttrain_loss: 15.326434135437012\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 873\ttrain_loss: 15.365214347839355\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 874\ttrain_loss: 15.453959465026855\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 875\ttrain_loss: 15.060624122619629\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 876\ttrain_loss: 15.202973365783691\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 877\ttrain_loss: 15.232357025146484\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 878\ttrain_loss: 15.22977066040039\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 879\ttrain_loss: 15.39933967590332\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 880\ttrain_loss: 15.389958381652832\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 881\ttrain_loss: 15.233089447021484\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 882\ttrain_loss: 15.20463752746582\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 883\ttrain_loss: 15.302212715148926\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 884\ttrain_loss: 15.264253616333008\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 885\ttrain_loss: 15.28993034362793\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 886\ttrain_loss: 15.438225746154785\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 887\ttrain_loss: 15.197212219238281\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 888\ttrain_loss: 15.214132308959961\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 889\ttrain_loss: 15.214031219482422\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 890\ttrain_loss: 15.159095764160156\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 891\ttrain_loss: 15.27562141418457\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 892\ttrain_loss: 15.175850868225098\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 893\ttrain_loss: 15.356104850769043\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 894\ttrain_loss: 15.305623054504395\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 895\ttrain_loss: 15.233080863952637\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 896\ttrain_loss: 15.159760475158691\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 897\ttrain_loss: 15.108166694641113\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 898\ttrain_loss: 15.46013355255127\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 899\ttrain_loss: 15.290139198303223\tval_loss: 15.26583480834961\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 900\ttrain_loss: 15.262404441833496\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 901\ttrain_loss: 15.137307167053223\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 902\ttrain_loss: 15.096561431884766\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 903\ttrain_loss: 15.230379104614258\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 904\ttrain_loss: 15.095739364624023\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 905\ttrain_loss: 15.097855567932129\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 906\ttrain_loss: 15.154483795166016\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 907\ttrain_loss: 15.270719528198242\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 908\ttrain_loss: 15.141153335571289\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 909\ttrain_loss: 15.372859954833984\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 910\ttrain_loss: 15.49283218383789\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 911\ttrain_loss: 15.228458404541016\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 912\ttrain_loss: 15.153573989868164\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 913\ttrain_loss: 15.40703010559082\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 914\ttrain_loss: 15.52649211883545\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 915\ttrain_loss: 15.233595848083496\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 916\ttrain_loss: 14.954521179199219\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 917\ttrain_loss: 15.34368896484375\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 918\ttrain_loss: 15.345942497253418\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 919\ttrain_loss: 15.264970779418945\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 920\ttrain_loss: 15.250978469848633\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 921\ttrain_loss: 15.263018608093262\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 922\ttrain_loss: 15.386819839477539\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 923\ttrain_loss: 15.084674835205078\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 924\ttrain_loss: 15.230026245117188\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 925\ttrain_loss: 15.306331634521484\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 926\ttrain_loss: 15.307145118713379\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 927\ttrain_loss: 15.268280982971191\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 928\ttrain_loss: 15.410195350646973\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 929\ttrain_loss: 15.298633575439453\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 930\ttrain_loss: 15.296659469604492\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 931\ttrain_loss: 15.398921966552734\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 932\ttrain_loss: 15.372828483581543\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 933\ttrain_loss: 15.205288887023926\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 934\ttrain_loss: 15.232760429382324\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 935\ttrain_loss: 15.0853853225708\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 936\ttrain_loss: 15.225096702575684\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 937\ttrain_loss: 15.352092742919922\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 938\ttrain_loss: 15.284650802612305\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 939\ttrain_loss: 15.26926040649414\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 940\ttrain_loss: 15.285188674926758\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 941\ttrain_loss: 15.072919845581055\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 942\ttrain_loss: 15.349621772766113\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 943\ttrain_loss: 15.226651191711426\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 944\ttrain_loss: 15.200316429138184\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 945\ttrain_loss: 15.066933631896973\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 946\ttrain_loss: 15.266711235046387\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 947\ttrain_loss: 15.166954040527344\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 948\ttrain_loss: 15.157832145690918\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 949\ttrain_loss: 15.2822265625\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 950\ttrain_loss: 15.42757797241211\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 951\ttrain_loss: 15.33791732788086\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 952\ttrain_loss: 15.22375774383545\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 953\ttrain_loss: 15.577387809753418\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 954\ttrain_loss: 15.315686225891113\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 955\ttrain_loss: 15.342784881591797\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 956\ttrain_loss: 15.450013160705566\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 957\ttrain_loss: 15.290005683898926\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 958\ttrain_loss: 15.209721565246582\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 959\ttrain_loss: 15.2108154296875\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 960\ttrain_loss: 15.081830024719238\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 961\ttrain_loss: 15.185276985168457\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 962\ttrain_loss: 15.206268310546875\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 963\ttrain_loss: 15.388504981994629\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 964\ttrain_loss: 15.205607414245605\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 965\ttrain_loss: 15.216845512390137\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 966\ttrain_loss: 15.127787590026855\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 967\ttrain_loss: 15.281753540039062\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 968\ttrain_loss: 15.32419204711914\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 969\ttrain_loss: 15.235684394836426\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 970\ttrain_loss: 15.2299222946167\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 971\ttrain_loss: 15.377828598022461\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 972\ttrain_loss: 15.171062469482422\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 973\ttrain_loss: 15.163068771362305\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 974\ttrain_loss: 15.208020210266113\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 975\ttrain_loss: 15.211396217346191\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 976\ttrain_loss: 15.43032455444336\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 977\ttrain_loss: 15.225512504577637\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 978\ttrain_loss: 15.206010818481445\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 979\ttrain_loss: 15.128443717956543\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 980\ttrain_loss: 15.22582721710205\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 981\ttrain_loss: 15.179801940917969\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 982\ttrain_loss: 15.260255813598633\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 983\ttrain_loss: 15.383622169494629\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 984\ttrain_loss: 15.262651443481445\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 985\ttrain_loss: 15.288886070251465\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 986\ttrain_loss: 15.458386421203613\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 987\ttrain_loss: 15.336969375610352\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 988\ttrain_loss: 15.185038566589355\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 989\ttrain_loss: 15.228235244750977\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 990\ttrain_loss: 15.314844131469727\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 991\ttrain_loss: 15.277555465698242\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 992\ttrain_loss: 15.521759033203125\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 993\ttrain_loss: 15.302691459655762\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 994\ttrain_loss: 15.199934005737305\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 995\ttrain_loss: 15.291655540466309\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 996\ttrain_loss: 15.441802978515625\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 997\ttrain_loss: 15.158052444458008\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 998\ttrain_loss: 15.351396560668945\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 999\ttrain_loss: 15.187225341796875\tval_loss: 15.26294994354248\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1000\ttrain_loss: 15.05747127532959\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1001\ttrain_loss: 15.222323417663574\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1002\ttrain_loss: 15.1393461227417\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1003\ttrain_loss: 15.304512023925781\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1004\ttrain_loss: 15.15776538848877\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1005\ttrain_loss: 15.363924980163574\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1006\ttrain_loss: 15.173368453979492\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1007\ttrain_loss: 15.317146301269531\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1008\ttrain_loss: 15.290067672729492\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1009\ttrain_loss: 15.236150741577148\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1010\ttrain_loss: 15.300870895385742\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1011\ttrain_loss: 15.282478332519531\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1012\ttrain_loss: 15.440104484558105\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1013\ttrain_loss: 15.230340003967285\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1014\ttrain_loss: 15.214208602905273\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1015\ttrain_loss: 15.274514198303223\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1016\ttrain_loss: 15.249266624450684\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1017\ttrain_loss: 15.2070894241333\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1018\ttrain_loss: 15.528817176818848\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1019\ttrain_loss: 15.170483589172363\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1020\ttrain_loss: 15.385151863098145\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1021\ttrain_loss: 15.267398834228516\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1022\ttrain_loss: 15.40971565246582\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1023\ttrain_loss: 15.321945190429688\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1024\ttrain_loss: 15.286316871643066\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1025\ttrain_loss: 15.302285194396973\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1026\ttrain_loss: 15.28524398803711\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1027\ttrain_loss: 15.263947486877441\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1028\ttrain_loss: 15.167773246765137\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1029\ttrain_loss: 15.183493614196777\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1030\ttrain_loss: 15.2321195602417\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1031\ttrain_loss: 15.284892082214355\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1032\ttrain_loss: 15.380287170410156\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1033\ttrain_loss: 15.24488353729248\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1034\ttrain_loss: 15.353076934814453\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1035\ttrain_loss: 15.069541931152344\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1036\ttrain_loss: 15.1779203414917\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1037\ttrain_loss: 15.353015899658203\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1038\ttrain_loss: 15.033567428588867\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1039\ttrain_loss: 15.324555397033691\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1040\ttrain_loss: 15.279022216796875\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1041\ttrain_loss: 15.195952415466309\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1042\ttrain_loss: 15.169943809509277\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1043\ttrain_loss: 15.264386177062988\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1044\ttrain_loss: 15.330337524414062\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1045\ttrain_loss: 15.109136581420898\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1046\ttrain_loss: 15.089556694030762\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1047\ttrain_loss: 15.1027193069458\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1048\ttrain_loss: 15.168432235717773\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1049\ttrain_loss: 14.95740795135498\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1050\ttrain_loss: 15.172314643859863\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1051\ttrain_loss: 15.2612886428833\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1052\ttrain_loss: 15.415962219238281\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1053\ttrain_loss: 15.229157447814941\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1054\ttrain_loss: 15.23487377166748\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1055\ttrain_loss: 15.298962593078613\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1056\ttrain_loss: 15.278508186340332\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1057\ttrain_loss: 15.303030014038086\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1058\ttrain_loss: 15.499388694763184\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1059\ttrain_loss: 15.023448944091797\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1060\ttrain_loss: 15.390764236450195\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1061\ttrain_loss: 15.166722297668457\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1062\ttrain_loss: 15.185961723327637\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1063\ttrain_loss: 15.58171558380127\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1064\ttrain_loss: 15.095762252807617\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1065\ttrain_loss: 15.271093368530273\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1066\ttrain_loss: 15.327970504760742\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1067\ttrain_loss: 15.12403678894043\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1068\ttrain_loss: 15.198436737060547\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1069\ttrain_loss: 15.243622779846191\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1070\ttrain_loss: 15.249105453491211\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1071\ttrain_loss: 15.419808387756348\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1072\ttrain_loss: 15.22737979888916\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1073\ttrain_loss: 15.257142066955566\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1074\ttrain_loss: 15.139391899108887\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1075\ttrain_loss: 15.133170127868652\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1076\ttrain_loss: 15.319683074951172\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1077\ttrain_loss: 15.173352241516113\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1078\ttrain_loss: 15.315558433532715\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1079\ttrain_loss: 15.278589248657227\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1080\ttrain_loss: 15.139074325561523\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1081\ttrain_loss: 15.30761432647705\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1082\ttrain_loss: 15.17013168334961\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1083\ttrain_loss: 15.182757377624512\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1084\ttrain_loss: 15.174883842468262\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1085\ttrain_loss: 15.467765808105469\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1086\ttrain_loss: 15.208257675170898\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1087\ttrain_loss: 15.068198204040527\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1088\ttrain_loss: 15.438712120056152\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1089\ttrain_loss: 15.382888793945312\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1090\ttrain_loss: 15.221365928649902\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1091\ttrain_loss: 15.277449607849121\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1092\ttrain_loss: 15.115280151367188\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1093\ttrain_loss: 15.142967224121094\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1094\ttrain_loss: 15.221614837646484\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1095\ttrain_loss: 15.190960884094238\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1096\ttrain_loss: 15.129403114318848\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1097\ttrain_loss: 15.142794609069824\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1098\ttrain_loss: 15.302523612976074\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1099\ttrain_loss: 15.17891788482666\tval_loss: 15.261128425598145\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1100\ttrain_loss: 15.173983573913574\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1101\ttrain_loss: 15.20732307434082\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1102\ttrain_loss: 15.296857833862305\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1103\ttrain_loss: 15.257291793823242\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1104\ttrain_loss: 15.549938201904297\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1105\ttrain_loss: 15.378982543945312\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1106\ttrain_loss: 15.239087104797363\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1107\ttrain_loss: 15.375595092773438\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1108\ttrain_loss: 15.358841896057129\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1109\ttrain_loss: 15.30966567993164\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1110\ttrain_loss: 15.230487823486328\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1111\ttrain_loss: 15.246996879577637\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1112\ttrain_loss: 15.21870231628418\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1113\ttrain_loss: 15.144625663757324\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1114\ttrain_loss: 15.443886756896973\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1115\ttrain_loss: 15.169731140136719\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1116\ttrain_loss: 15.235097885131836\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1117\ttrain_loss: 15.353840827941895\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1118\ttrain_loss: 15.189650535583496\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1119\ttrain_loss: 15.149428367614746\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1120\ttrain_loss: 15.190505981445312\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1121\ttrain_loss: 15.1600341796875\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1122\ttrain_loss: 15.323158264160156\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1123\ttrain_loss: 15.16242790222168\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1124\ttrain_loss: 15.237120628356934\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1125\ttrain_loss: 15.099672317504883\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1126\ttrain_loss: 15.104331970214844\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1127\ttrain_loss: 15.178011894226074\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1128\ttrain_loss: 15.322834968566895\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1129\ttrain_loss: 15.22717571258545\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1130\ttrain_loss: 15.201335906982422\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1131\ttrain_loss: 15.104104042053223\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1132\ttrain_loss: 15.309020042419434\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1133\ttrain_loss: 15.2617826461792\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1134\ttrain_loss: 15.270157814025879\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1135\ttrain_loss: 15.276511192321777\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1136\ttrain_loss: 15.229606628417969\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1137\ttrain_loss: 15.21731185913086\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1138\ttrain_loss: 15.343252182006836\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1139\ttrain_loss: 15.11937427520752\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1140\ttrain_loss: 15.105113983154297\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1141\ttrain_loss: 15.312785148620605\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1142\ttrain_loss: 15.300370216369629\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1143\ttrain_loss: 15.239734649658203\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1144\ttrain_loss: 15.26895809173584\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1145\ttrain_loss: 15.276167869567871\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1146\ttrain_loss: 15.250870704650879\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1147\ttrain_loss: 15.10103702545166\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1148\ttrain_loss: 15.315027236938477\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1149\ttrain_loss: 15.450791358947754\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1150\ttrain_loss: 15.270198822021484\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1151\ttrain_loss: 15.33064079284668\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1152\ttrain_loss: 15.267989158630371\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1153\ttrain_loss: 15.292643547058105\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1154\ttrain_loss: 15.216018676757812\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1155\ttrain_loss: 15.340996742248535\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1156\ttrain_loss: 15.147254943847656\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1157\ttrain_loss: 15.185351371765137\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1158\ttrain_loss: 15.279888153076172\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1159\ttrain_loss: 15.349623680114746\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1160\ttrain_loss: 15.483871459960938\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1161\ttrain_loss: 15.46732234954834\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1162\ttrain_loss: 15.272371292114258\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1163\ttrain_loss: 15.194028854370117\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1164\ttrain_loss: 15.277031898498535\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1165\ttrain_loss: 15.34293270111084\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1166\ttrain_loss: 15.33809757232666\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1167\ttrain_loss: 15.378149032592773\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1168\ttrain_loss: 15.32584285736084\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1169\ttrain_loss: 15.208450317382812\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1170\ttrain_loss: 15.189715385437012\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1171\ttrain_loss: 15.296661376953125\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1172\ttrain_loss: 15.386238098144531\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1173\ttrain_loss: 15.088105201721191\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1174\ttrain_loss: 15.249939918518066\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1175\ttrain_loss: 15.188446998596191\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1176\ttrain_loss: 15.32103157043457\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1177\ttrain_loss: 15.330902099609375\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1178\ttrain_loss: 15.454132080078125\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1179\ttrain_loss: 15.257158279418945\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1180\ttrain_loss: 15.236106872558594\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1181\ttrain_loss: 15.336856842041016\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1182\ttrain_loss: 15.361116409301758\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1183\ttrain_loss: 15.390552520751953\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1184\ttrain_loss: 15.131640434265137\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1185\ttrain_loss: 15.047661781311035\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1186\ttrain_loss: 15.447821617126465\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1187\ttrain_loss: 15.377938270568848\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1188\ttrain_loss: 15.351713180541992\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1189\ttrain_loss: 15.142755508422852\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1190\ttrain_loss: 15.311498641967773\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1191\ttrain_loss: 15.381251335144043\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1192\ttrain_loss: 15.103198051452637\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1193\ttrain_loss: 15.215645790100098\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1194\ttrain_loss: 15.239301681518555\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1195\ttrain_loss: 15.547654151916504\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1196\ttrain_loss: 15.233552932739258\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1197\ttrain_loss: 15.344776153564453\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1198\ttrain_loss: 15.466893196105957\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1199\ttrain_loss: 15.283388137817383\tval_loss: 15.259836196899414\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1200\ttrain_loss: 15.539617538452148\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1201\ttrain_loss: 15.285348892211914\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1202\ttrain_loss: 15.414446830749512\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1203\ttrain_loss: 15.314051628112793\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1204\ttrain_loss: 15.254956245422363\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1205\ttrain_loss: 15.413802146911621\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1206\ttrain_loss: 15.302738189697266\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1207\ttrain_loss: 15.473931312561035\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1208\ttrain_loss: 15.107949256896973\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1209\ttrain_loss: 15.303887367248535\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1210\ttrain_loss: 15.11571979522705\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1211\ttrain_loss: 15.153368949890137\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1212\ttrain_loss: 15.17126750946045\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1213\ttrain_loss: 15.54278564453125\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1214\ttrain_loss: 15.279854774475098\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1215\ttrain_loss: 15.120760917663574\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1216\ttrain_loss: 15.430520057678223\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1217\ttrain_loss: 15.203173637390137\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1218\ttrain_loss: 15.223878860473633\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1219\ttrain_loss: 15.253928184509277\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1220\ttrain_loss: 15.22053337097168\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1221\ttrain_loss: 15.31603717803955\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1222\ttrain_loss: 15.329510688781738\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1223\ttrain_loss: 15.392236709594727\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1224\ttrain_loss: 15.384631156921387\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1225\ttrain_loss: 15.34642219543457\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1226\ttrain_loss: 15.384329795837402\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1227\ttrain_loss: 15.228046417236328\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1228\ttrain_loss: 15.38421630859375\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1229\ttrain_loss: 15.39083194732666\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1230\ttrain_loss: 15.203384399414062\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1231\ttrain_loss: 15.160964012145996\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1232\ttrain_loss: 15.002366065979004\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1233\ttrain_loss: 15.313343048095703\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1234\ttrain_loss: 15.378823280334473\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1235\ttrain_loss: 15.185859680175781\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1236\ttrain_loss: 15.25119400024414\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1237\ttrain_loss: 15.184911727905273\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1238\ttrain_loss: 15.223250389099121\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1239\ttrain_loss: 15.336867332458496\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1240\ttrain_loss: 15.207723617553711\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1241\ttrain_loss: 15.376785278320312\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1242\ttrain_loss: 15.156134605407715\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1243\ttrain_loss: 15.265169143676758\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1244\ttrain_loss: 15.187891006469727\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1245\ttrain_loss: 15.277917861938477\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1246\ttrain_loss: 15.324102401733398\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1247\ttrain_loss: 15.236331939697266\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1248\ttrain_loss: 15.214322090148926\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1249\ttrain_loss: 15.335487365722656\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1250\ttrain_loss: 15.355931282043457\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1251\ttrain_loss: 15.318952560424805\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1252\ttrain_loss: 15.1972017288208\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1253\ttrain_loss: 15.305825233459473\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1254\ttrain_loss: 15.29526138305664\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1255\ttrain_loss: 15.25167179107666\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1256\ttrain_loss: 15.236632347106934\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1257\ttrain_loss: 15.24813175201416\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1258\ttrain_loss: 15.146598815917969\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1259\ttrain_loss: 15.244997024536133\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1260\ttrain_loss: 15.197736740112305\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1261\ttrain_loss: 15.274829864501953\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1262\ttrain_loss: 15.227852821350098\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1263\ttrain_loss: 15.316184043884277\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1264\ttrain_loss: 15.290762901306152\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1265\ttrain_loss: 15.387373924255371\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1266\ttrain_loss: 15.196362495422363\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1267\ttrain_loss: 15.163472175598145\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1268\ttrain_loss: 15.42998218536377\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1269\ttrain_loss: 15.178997039794922\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1270\ttrain_loss: 15.207974433898926\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1271\ttrain_loss: 15.082554817199707\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1272\ttrain_loss: 15.32561206817627\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1273\ttrain_loss: 15.25023365020752\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1274\ttrain_loss: 15.422684669494629\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1275\ttrain_loss: 15.358285903930664\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1276\ttrain_loss: 15.192488670349121\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1277\ttrain_loss: 15.241494178771973\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1278\ttrain_loss: 15.14330768585205\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1279\ttrain_loss: 15.269023895263672\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1280\ttrain_loss: 15.094216346740723\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1281\ttrain_loss: 15.383308410644531\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1282\ttrain_loss: 15.166175842285156\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1283\ttrain_loss: 15.433731079101562\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1284\ttrain_loss: 15.199972152709961\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1285\ttrain_loss: 15.254903793334961\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1286\ttrain_loss: 15.322293281555176\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1287\ttrain_loss: 15.44716739654541\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1288\ttrain_loss: 15.259419441223145\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1289\ttrain_loss: 15.188736915588379\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1290\ttrain_loss: 15.263174057006836\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1291\ttrain_loss: 15.213613510131836\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1292\ttrain_loss: 15.42906665802002\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1293\ttrain_loss: 15.289971351623535\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1294\ttrain_loss: 15.289093971252441\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1295\ttrain_loss: 15.373663902282715\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1296\ttrain_loss: 15.197110176086426\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1297\ttrain_loss: 15.253897666931152\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1298\ttrain_loss: 15.277851104736328\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1299\ttrain_loss: 15.48206615447998\tval_loss: 15.257306098937988\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1300\ttrain_loss: 15.061681747436523\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1301\ttrain_loss: 15.285768508911133\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1302\ttrain_loss: 15.314945220947266\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1303\ttrain_loss: 15.113263130187988\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1304\ttrain_loss: 15.008076667785645\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1305\ttrain_loss: 15.338593482971191\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1306\ttrain_loss: 15.24899673461914\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1307\ttrain_loss: 15.174565315246582\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1308\ttrain_loss: 15.171636581420898\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1309\ttrain_loss: 15.285581588745117\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1310\ttrain_loss: 15.45836353302002\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1311\ttrain_loss: 15.329258918762207\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1312\ttrain_loss: 15.218774795532227\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1313\ttrain_loss: 15.323984146118164\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1314\ttrain_loss: 15.288968086242676\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1315\ttrain_loss: 15.34793758392334\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1316\ttrain_loss: 15.309276580810547\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1317\ttrain_loss: 15.283157348632812\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1318\ttrain_loss: 15.209010124206543\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1319\ttrain_loss: 15.411765098571777\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1320\ttrain_loss: 15.20186710357666\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1321\ttrain_loss: 15.10775089263916\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1322\ttrain_loss: 15.175397872924805\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1323\ttrain_loss: 15.183830261230469\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1324\ttrain_loss: 15.130537033081055\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1325\ttrain_loss: 15.287724494934082\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1326\ttrain_loss: 15.147897720336914\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1327\ttrain_loss: 15.074945449829102\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1328\ttrain_loss: 15.420553207397461\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1329\ttrain_loss: 15.228043556213379\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1330\ttrain_loss: 15.285470008850098\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1331\ttrain_loss: 15.153131484985352\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1332\ttrain_loss: 15.27431583404541\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1333\ttrain_loss: 15.314836502075195\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1334\ttrain_loss: 15.411040306091309\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1335\ttrain_loss: 15.404619216918945\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1336\ttrain_loss: 15.229238510131836\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1337\ttrain_loss: 15.488080024719238\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1338\ttrain_loss: 15.128174781799316\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1339\ttrain_loss: 15.294918060302734\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1340\ttrain_loss: 15.23534870147705\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1341\ttrain_loss: 15.366788864135742\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1342\ttrain_loss: 15.201919555664062\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1343\ttrain_loss: 15.256516456604004\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1344\ttrain_loss: 15.314488410949707\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1345\ttrain_loss: 15.510607719421387\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1346\ttrain_loss: 15.078445434570312\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1347\ttrain_loss: 15.466972351074219\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1348\ttrain_loss: 15.279876708984375\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1349\ttrain_loss: 15.3378267288208\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1350\ttrain_loss: 15.101401329040527\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1351\ttrain_loss: 15.257431030273438\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1352\ttrain_loss: 15.125856399536133\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1353\ttrain_loss: 15.233433723449707\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1354\ttrain_loss: 15.376031875610352\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1355\ttrain_loss: 15.364514350891113\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1356\ttrain_loss: 15.292130470275879\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1357\ttrain_loss: 15.282240867614746\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1358\ttrain_loss: 15.275195121765137\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1359\ttrain_loss: 15.182156562805176\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1360\ttrain_loss: 15.39787769317627\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1361\ttrain_loss: 15.297551155090332\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1362\ttrain_loss: 15.195843696594238\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1363\ttrain_loss: 15.186471939086914\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1364\ttrain_loss: 15.257905960083008\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1365\ttrain_loss: 15.360349655151367\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1366\ttrain_loss: 15.211308479309082\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1367\ttrain_loss: 15.452857971191406\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1368\ttrain_loss: 15.429290771484375\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1369\ttrain_loss: 15.349023818969727\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1370\ttrain_loss: 15.12850284576416\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1371\ttrain_loss: 15.379438400268555\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1372\ttrain_loss: 15.320860862731934\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1373\ttrain_loss: 15.227653503417969\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1374\ttrain_loss: 15.324981689453125\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1375\ttrain_loss: 15.074728012084961\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1376\ttrain_loss: 15.30307388305664\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1377\ttrain_loss: 15.071876525878906\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1378\ttrain_loss: 15.425975799560547\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1379\ttrain_loss: 15.223697662353516\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1380\ttrain_loss: 15.204070091247559\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1381\ttrain_loss: 15.06439208984375\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1382\ttrain_loss: 15.268495559692383\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1383\ttrain_loss: 15.46645450592041\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1384\ttrain_loss: 15.214554786682129\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1385\ttrain_loss: 15.210097312927246\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1386\ttrain_loss: 15.221628189086914\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1387\ttrain_loss: 15.445945739746094\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1388\ttrain_loss: 15.404596328735352\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1389\ttrain_loss: 15.245851516723633\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1390\ttrain_loss: 15.417070388793945\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1391\ttrain_loss: 15.334842681884766\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1392\ttrain_loss: 15.36872673034668\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1393\ttrain_loss: 15.21753978729248\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1394\ttrain_loss: 15.158235549926758\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1395\ttrain_loss: 15.196258544921875\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1396\ttrain_loss: 15.268006324768066\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1397\ttrain_loss: 15.243435859680176\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1398\ttrain_loss: 15.299079895019531\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1399\ttrain_loss: 15.380912780761719\tval_loss: 15.255044937133789\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1400\ttrain_loss: 15.350119590759277\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1401\ttrain_loss: 15.118429183959961\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1402\ttrain_loss: 14.970088005065918\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1403\ttrain_loss: 15.219756126403809\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1404\ttrain_loss: 15.4004545211792\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1405\ttrain_loss: 15.256158828735352\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1406\ttrain_loss: 15.313750267028809\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1407\ttrain_loss: 15.436519622802734\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1408\ttrain_loss: 15.351184844970703\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1409\ttrain_loss: 15.204633712768555\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1410\ttrain_loss: 15.241537094116211\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1411\ttrain_loss: 15.420708656311035\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1412\ttrain_loss: 15.144485473632812\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1413\ttrain_loss: 15.257505416870117\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1414\ttrain_loss: 15.261337280273438\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1415\ttrain_loss: 15.30665397644043\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1416\ttrain_loss: 15.371060371398926\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1417\ttrain_loss: 15.333793640136719\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1418\ttrain_loss: 15.073278427124023\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1419\ttrain_loss: 15.386185646057129\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1420\ttrain_loss: 15.082255363464355\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1421\ttrain_loss: 15.208037376403809\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1422\ttrain_loss: 15.265641212463379\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1423\ttrain_loss: 15.396056175231934\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1424\ttrain_loss: 15.325684547424316\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1425\ttrain_loss: 15.423337936401367\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1426\ttrain_loss: 15.391780853271484\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1427\ttrain_loss: 15.278411865234375\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1428\ttrain_loss: 15.386347770690918\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1429\ttrain_loss: 15.381514549255371\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1430\ttrain_loss: 15.25058650970459\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1431\ttrain_loss: 15.411911010742188\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1432\ttrain_loss: 15.416857719421387\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1433\ttrain_loss: 15.362725257873535\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1434\ttrain_loss: 15.273772239685059\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1435\ttrain_loss: 15.371393203735352\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1436\ttrain_loss: 15.259222030639648\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1437\ttrain_loss: 15.11252212524414\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1438\ttrain_loss: 15.202407836914062\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1439\ttrain_loss: 15.248490333557129\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1440\ttrain_loss: 15.129539489746094\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1441\ttrain_loss: 15.293020248413086\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1442\ttrain_loss: 15.155058860778809\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1443\ttrain_loss: 15.295730590820312\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1444\ttrain_loss: 15.089534759521484\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1445\ttrain_loss: 15.426803588867188\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1446\ttrain_loss: 15.419145584106445\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1447\ttrain_loss: 15.348462104797363\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1448\ttrain_loss: 15.266260147094727\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1449\ttrain_loss: 15.137983322143555\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1450\ttrain_loss: 15.201740264892578\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1451\ttrain_loss: 15.316451072692871\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1452\ttrain_loss: 15.26384162902832\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1453\ttrain_loss: 15.105260848999023\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1454\ttrain_loss: 15.12428092956543\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1455\ttrain_loss: 15.186861991882324\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1456\ttrain_loss: 15.196432113647461\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1457\ttrain_loss: 15.137679100036621\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1458\ttrain_loss: 15.085709571838379\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1459\ttrain_loss: 15.204886436462402\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1460\ttrain_loss: 15.191726684570312\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1461\ttrain_loss: 15.12413501739502\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1462\ttrain_loss: 15.426338195800781\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1463\ttrain_loss: 15.388110160827637\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1464\ttrain_loss: 15.199061393737793\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1465\ttrain_loss: 15.251593589782715\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1466\ttrain_loss: 15.309656143188477\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1467\ttrain_loss: 15.288761138916016\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1468\ttrain_loss: 15.331036567687988\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1469\ttrain_loss: 15.028000831604004\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1470\ttrain_loss: 15.185861587524414\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1471\ttrain_loss: 15.299067497253418\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1472\ttrain_loss: 15.172316551208496\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1473\ttrain_loss: 15.46214771270752\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1474\ttrain_loss: 15.283773422241211\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1475\ttrain_loss: 15.245304107666016\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1476\ttrain_loss: 15.368768692016602\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1477\ttrain_loss: 15.163146018981934\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1478\ttrain_loss: 15.188644409179688\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1479\ttrain_loss: 15.267251968383789\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1480\ttrain_loss: 15.372909545898438\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1481\ttrain_loss: 15.13228988647461\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1482\ttrain_loss: 15.249558448791504\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1483\ttrain_loss: 15.172730445861816\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1484\ttrain_loss: 15.174489974975586\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1485\ttrain_loss: 15.455366134643555\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1486\ttrain_loss: 15.112661361694336\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1487\ttrain_loss: 15.22354793548584\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1488\ttrain_loss: 15.301112174987793\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1489\ttrain_loss: 15.135988235473633\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1490\ttrain_loss: 15.237998008728027\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1491\ttrain_loss: 15.25405502319336\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1492\ttrain_loss: 15.109821319580078\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1493\ttrain_loss: 15.040600776672363\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1494\ttrain_loss: 15.198790550231934\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1495\ttrain_loss: 15.128905296325684\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1496\ttrain_loss: 15.244275093078613\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1497\ttrain_loss: 15.197237014770508\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1498\ttrain_loss: 15.2202787399292\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1499\ttrain_loss: 15.403230667114258\tval_loss: 15.253691673278809\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1500\ttrain_loss: 15.274273872375488\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1501\ttrain_loss: 15.303741455078125\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1502\ttrain_loss: 15.278949737548828\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1503\ttrain_loss: 15.342995643615723\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1504\ttrain_loss: 15.224549293518066\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1505\ttrain_loss: 15.427841186523438\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1506\ttrain_loss: 15.294340133666992\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1507\ttrain_loss: 15.21803092956543\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1508\ttrain_loss: 15.2748384475708\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1509\ttrain_loss: 15.23349380493164\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1510\ttrain_loss: 15.076024055480957\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1511\ttrain_loss: 15.131129264831543\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1512\ttrain_loss: 15.42551326751709\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1513\ttrain_loss: 15.364974975585938\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1514\ttrain_loss: 15.371848106384277\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1515\ttrain_loss: 15.293789863586426\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1516\ttrain_loss: 15.26438045501709\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1517\ttrain_loss: 15.172955513000488\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1518\ttrain_loss: 15.192066192626953\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1519\ttrain_loss: 15.316648483276367\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1520\ttrain_loss: 15.301980972290039\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1521\ttrain_loss: 15.50827407836914\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1522\ttrain_loss: 15.320926666259766\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1523\ttrain_loss: 15.389809608459473\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1524\ttrain_loss: 15.126537322998047\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1525\ttrain_loss: 15.239386558532715\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1526\ttrain_loss: 15.277389526367188\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1527\ttrain_loss: 15.201725006103516\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1528\ttrain_loss: 15.493586540222168\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1529\ttrain_loss: 15.299880027770996\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1530\ttrain_loss: 15.075098037719727\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1531\ttrain_loss: 15.158281326293945\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1532\ttrain_loss: 15.199874877929688\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1533\ttrain_loss: 15.1100492477417\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1534\ttrain_loss: 15.345934867858887\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1535\ttrain_loss: 15.329242706298828\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1536\ttrain_loss: 15.06700325012207\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1537\ttrain_loss: 15.282909393310547\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1538\ttrain_loss: 15.310288429260254\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1539\ttrain_loss: 15.365752220153809\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1540\ttrain_loss: 15.131476402282715\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1541\ttrain_loss: 15.087641716003418\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1542\ttrain_loss: 15.359143257141113\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1543\ttrain_loss: 15.246384620666504\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1544\ttrain_loss: 15.189598083496094\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1545\ttrain_loss: 15.438006401062012\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1546\ttrain_loss: 15.151795387268066\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1547\ttrain_loss: 15.379705429077148\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1548\ttrain_loss: 15.274785995483398\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1549\ttrain_loss: 15.355945587158203\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1550\ttrain_loss: 15.14297866821289\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1551\ttrain_loss: 15.134020805358887\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1552\ttrain_loss: 15.55323314666748\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1553\ttrain_loss: 15.296243667602539\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1554\ttrain_loss: 15.15766716003418\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1555\ttrain_loss: 15.36401653289795\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1556\ttrain_loss: 15.28355884552002\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1557\ttrain_loss: 15.29969310760498\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1558\ttrain_loss: 15.169230461120605\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1559\ttrain_loss: 15.237462043762207\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1560\ttrain_loss: 15.230456352233887\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1561\ttrain_loss: 15.060770988464355\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1562\ttrain_loss: 15.265732765197754\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1563\ttrain_loss: 15.239964485168457\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1564\ttrain_loss: 15.267802238464355\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1565\ttrain_loss: 15.204842567443848\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1566\ttrain_loss: 15.1467924118042\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1567\ttrain_loss: 15.183894157409668\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1568\ttrain_loss: 15.353682518005371\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1569\ttrain_loss: 15.288323402404785\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1570\ttrain_loss: 15.205174446105957\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1571\ttrain_loss: 15.41524887084961\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1572\ttrain_loss: 15.312854766845703\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1573\ttrain_loss: 15.114370346069336\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1574\ttrain_loss: 15.280203819274902\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1575\ttrain_loss: 15.221292495727539\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1576\ttrain_loss: 15.10472297668457\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1577\ttrain_loss: 15.283452987670898\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1578\ttrain_loss: 15.206659317016602\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1579\ttrain_loss: 15.177567481994629\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1580\ttrain_loss: 15.236106872558594\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1581\ttrain_loss: 15.136561393737793\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1582\ttrain_loss: 15.317707061767578\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1583\ttrain_loss: 15.285308837890625\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1584\ttrain_loss: 15.263382911682129\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1585\ttrain_loss: 15.366189956665039\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1586\ttrain_loss: 15.264383316040039\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1587\ttrain_loss: 15.132454872131348\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1588\ttrain_loss: 15.322554588317871\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1589\ttrain_loss: 15.314526557922363\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1590\ttrain_loss: 15.105061531066895\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1591\ttrain_loss: 15.200264930725098\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1592\ttrain_loss: 15.316266059875488\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1593\ttrain_loss: 15.160606384277344\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1594\ttrain_loss: 15.13381290435791\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1595\ttrain_loss: 15.175257682800293\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1596\ttrain_loss: 15.121091842651367\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1597\ttrain_loss: 15.321348190307617\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1598\ttrain_loss: 15.338716506958008\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1599\ttrain_loss: 15.380281448364258\tval_loss: 15.252738952636719\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1600\ttrain_loss: 15.181907653808594\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1601\ttrain_loss: 15.244120597839355\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1602\ttrain_loss: 15.36026382446289\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1603\ttrain_loss: 15.28459644317627\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1604\ttrain_loss: 15.187088966369629\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1605\ttrain_loss: 15.28927230834961\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1606\ttrain_loss: 15.126572608947754\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1607\ttrain_loss: 15.194199562072754\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1608\ttrain_loss: 15.235245704650879\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1609\ttrain_loss: 15.218727111816406\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1610\ttrain_loss: 15.2096529006958\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1611\ttrain_loss: 15.413004875183105\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1612\ttrain_loss: 15.247806549072266\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1613\ttrain_loss: 15.278157234191895\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1614\ttrain_loss: 15.289983749389648\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1615\ttrain_loss: 15.332831382751465\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1616\ttrain_loss: 15.237593650817871\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1617\ttrain_loss: 15.147090911865234\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1618\ttrain_loss: 15.258875846862793\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1619\ttrain_loss: 15.206656455993652\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1620\ttrain_loss: 15.127398490905762\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1621\ttrain_loss: 15.118876457214355\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1622\ttrain_loss: 15.134459495544434\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1623\ttrain_loss: 15.180889129638672\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1624\ttrain_loss: 15.445245742797852\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1625\ttrain_loss: 15.182132720947266\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1626\ttrain_loss: 15.269387245178223\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1627\ttrain_loss: 15.481742858886719\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1628\ttrain_loss: 15.170852661132812\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1629\ttrain_loss: 15.164326667785645\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1630\ttrain_loss: 15.259658813476562\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1631\ttrain_loss: 15.219989776611328\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1632\ttrain_loss: 15.232486724853516\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1633\ttrain_loss: 15.258671760559082\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1634\ttrain_loss: 15.381595611572266\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1635\ttrain_loss: 15.220453262329102\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1636\ttrain_loss: 15.282873153686523\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1637\ttrain_loss: 15.342267036437988\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1638\ttrain_loss: 15.101724624633789\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1639\ttrain_loss: 15.094026565551758\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1640\ttrain_loss: 15.28735637664795\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1641\ttrain_loss: 15.149417877197266\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1642\ttrain_loss: 15.154457092285156\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1643\ttrain_loss: 15.308553695678711\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1644\ttrain_loss: 15.394030570983887\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1645\ttrain_loss: 15.212465286254883\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1646\ttrain_loss: 15.318784713745117\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1647\ttrain_loss: 15.036945343017578\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1648\ttrain_loss: 15.144352912902832\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1649\ttrain_loss: 15.377898216247559\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1650\ttrain_loss: 15.20645809173584\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1651\ttrain_loss: 15.252853393554688\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1652\ttrain_loss: 15.16324520111084\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1653\ttrain_loss: 15.199088096618652\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1654\ttrain_loss: 15.237486839294434\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1655\ttrain_loss: 15.21370792388916\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1656\ttrain_loss: 15.226967811584473\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1657\ttrain_loss: 15.314287185668945\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1658\ttrain_loss: 15.160171508789062\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1659\ttrain_loss: 15.154536247253418\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1660\ttrain_loss: 15.33384895324707\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1661\ttrain_loss: 15.356868743896484\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1662\ttrain_loss: 15.189382553100586\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1663\ttrain_loss: 15.32502555847168\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1664\ttrain_loss: 15.289426803588867\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1665\ttrain_loss: 15.309476852416992\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1666\ttrain_loss: 15.113337516784668\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1667\ttrain_loss: 15.121984481811523\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1668\ttrain_loss: 15.155628204345703\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1669\ttrain_loss: 15.381486892700195\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1670\ttrain_loss: 15.35516357421875\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1671\ttrain_loss: 15.077142715454102\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1672\ttrain_loss: 15.208809852600098\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1673\ttrain_loss: 15.24941349029541\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1674\ttrain_loss: 15.138531684875488\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1675\ttrain_loss: 15.221449851989746\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1676\ttrain_loss: 15.262693405151367\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1677\ttrain_loss: 15.162409782409668\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1678\ttrain_loss: 15.20775032043457\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1679\ttrain_loss: 15.081886291503906\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1680\ttrain_loss: 15.429266929626465\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1681\ttrain_loss: 15.35231876373291\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1682\ttrain_loss: 15.351069450378418\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1683\ttrain_loss: 15.395994186401367\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1684\ttrain_loss: 15.304441452026367\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1685\ttrain_loss: 15.34017562866211\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1686\ttrain_loss: 15.539484024047852\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1687\ttrain_loss: 15.268096923828125\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1688\ttrain_loss: 15.23237133026123\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1689\ttrain_loss: 15.261076927185059\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1690\ttrain_loss: 15.466678619384766\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1691\ttrain_loss: 15.06887435913086\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1692\ttrain_loss: 15.258223533630371\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1693\ttrain_loss: 15.372184753417969\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1694\ttrain_loss: 15.414913177490234\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1695\ttrain_loss: 15.220008850097656\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1696\ttrain_loss: 15.389904022216797\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1697\ttrain_loss: 15.158219337463379\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1698\ttrain_loss: 15.114609718322754\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1699\ttrain_loss: 15.21762752532959\tval_loss: 15.251588821411133\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1700\ttrain_loss: 15.212424278259277\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1701\ttrain_loss: 15.190200805664062\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1702\ttrain_loss: 15.212674140930176\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1703\ttrain_loss: 15.308403968811035\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1704\ttrain_loss: 15.291584014892578\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1705\ttrain_loss: 15.160700798034668\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1706\ttrain_loss: 15.467695236206055\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1707\ttrain_loss: 15.235408782958984\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1708\ttrain_loss: 15.262179374694824\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1709\ttrain_loss: 15.183618545532227\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1710\ttrain_loss: 15.262604713439941\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1711\ttrain_loss: 15.144318580627441\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1712\ttrain_loss: 15.093854904174805\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1713\ttrain_loss: 15.181694030761719\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1714\ttrain_loss: 15.23507022857666\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1715\ttrain_loss: 15.179070472717285\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1716\ttrain_loss: 15.112386703491211\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1717\ttrain_loss: 15.288492202758789\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1718\ttrain_loss: 15.110283851623535\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1719\ttrain_loss: 15.18249225616455\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1720\ttrain_loss: 15.150922775268555\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1721\ttrain_loss: 15.269684791564941\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1722\ttrain_loss: 15.54501724243164\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1723\ttrain_loss: 15.252474784851074\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1724\ttrain_loss: 15.164019584655762\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1725\ttrain_loss: 15.191241264343262\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1726\ttrain_loss: 15.186928749084473\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1727\ttrain_loss: 15.117835998535156\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1728\ttrain_loss: 15.2794189453125\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1729\ttrain_loss: 15.357138633728027\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1730\ttrain_loss: 15.155193328857422\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1731\ttrain_loss: 15.332233428955078\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1732\ttrain_loss: 15.261474609375\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1733\ttrain_loss: 15.19915771484375\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1734\ttrain_loss: 15.276510238647461\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1735\ttrain_loss: 15.302709579467773\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1736\ttrain_loss: 15.219344139099121\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1737\ttrain_loss: 15.208065032958984\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1738\ttrain_loss: 15.328290939331055\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1739\ttrain_loss: 15.196695327758789\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1740\ttrain_loss: 15.22652530670166\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1741\ttrain_loss: 15.280295372009277\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1742\ttrain_loss: 15.260200500488281\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1743\ttrain_loss: 15.157557487487793\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1744\ttrain_loss: 15.199244499206543\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1745\ttrain_loss: 15.243130683898926\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1746\ttrain_loss: 15.243908882141113\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1747\ttrain_loss: 15.295071601867676\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1748\ttrain_loss: 15.189016342163086\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1749\ttrain_loss: 15.27242660522461\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1750\ttrain_loss: 15.301143646240234\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1751\ttrain_loss: 15.182036399841309\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1752\ttrain_loss: 15.298369407653809\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1753\ttrain_loss: 15.416919708251953\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1754\ttrain_loss: 15.466683387756348\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1755\ttrain_loss: 15.167981147766113\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1756\ttrain_loss: 15.126209259033203\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1757\ttrain_loss: 15.263202667236328\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1758\ttrain_loss: 15.189438819885254\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1759\ttrain_loss: 15.160314559936523\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1760\ttrain_loss: 15.394244194030762\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1761\ttrain_loss: 15.326501846313477\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1762\ttrain_loss: 15.424870491027832\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1763\ttrain_loss: 15.376160621643066\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1764\ttrain_loss: 15.323461532592773\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1765\ttrain_loss: 15.331130027770996\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1766\ttrain_loss: 15.156631469726562\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1767\ttrain_loss: 15.148056983947754\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1768\ttrain_loss: 15.320474624633789\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1769\ttrain_loss: 15.282273292541504\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1770\ttrain_loss: 15.13411808013916\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1771\ttrain_loss: 15.353507041931152\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1772\ttrain_loss: 15.21904182434082\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1773\ttrain_loss: 15.25306510925293\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1774\ttrain_loss: 15.23699951171875\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1775\ttrain_loss: 15.20891284942627\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1776\ttrain_loss: 15.315089225769043\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1777\ttrain_loss: 15.297119140625\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1778\ttrain_loss: 15.31139087677002\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1779\ttrain_loss: 15.395090103149414\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1780\ttrain_loss: 15.204798698425293\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1781\ttrain_loss: 15.177947998046875\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1782\ttrain_loss: 15.289234161376953\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1783\ttrain_loss: 15.06342601776123\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1784\ttrain_loss: 15.366743087768555\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1785\ttrain_loss: 15.253399848937988\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1786\ttrain_loss: 15.322206497192383\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1787\ttrain_loss: 15.345538139343262\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1788\ttrain_loss: 15.287748336791992\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1789\ttrain_loss: 15.397883415222168\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1790\ttrain_loss: 15.253033638000488\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1791\ttrain_loss: 15.22904109954834\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1792\ttrain_loss: 15.276178359985352\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1793\ttrain_loss: 15.124431610107422\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1794\ttrain_loss: 15.318802833557129\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1795\ttrain_loss: 15.161816596984863\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1796\ttrain_loss: 15.333810806274414\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1797\ttrain_loss: 15.187573432922363\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1798\ttrain_loss: 15.328601837158203\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1799\ttrain_loss: 15.278714179992676\tval_loss: 15.250239372253418\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1800\ttrain_loss: 15.307437896728516\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1801\ttrain_loss: 15.267807960510254\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1802\ttrain_loss: 15.192773818969727\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1803\ttrain_loss: 15.16369342803955\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1804\ttrain_loss: 15.232137680053711\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1805\ttrain_loss: 15.127613067626953\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1806\ttrain_loss: 15.355253219604492\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1807\ttrain_loss: 15.290227890014648\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1808\ttrain_loss: 15.425734519958496\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1809\ttrain_loss: 15.037948608398438\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1810\ttrain_loss: 15.183537483215332\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1811\ttrain_loss: 15.272346496582031\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1812\ttrain_loss: 15.180684089660645\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1813\ttrain_loss: 15.148633003234863\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1814\ttrain_loss: 15.309678077697754\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1815\ttrain_loss: 15.117000579833984\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1816\ttrain_loss: 15.317855834960938\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1817\ttrain_loss: 15.130867004394531\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1818\ttrain_loss: 15.030364036560059\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1819\ttrain_loss: 15.131714820861816\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1820\ttrain_loss: 15.203452110290527\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1821\ttrain_loss: 15.187387466430664\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1822\ttrain_loss: 15.160032272338867\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1823\ttrain_loss: 15.173666000366211\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1824\ttrain_loss: 15.13583755493164\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1825\ttrain_loss: 15.21312427520752\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1826\ttrain_loss: 15.291123390197754\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1827\ttrain_loss: 15.414094924926758\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1828\ttrain_loss: 15.3085355758667\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1829\ttrain_loss: 15.16655445098877\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1830\ttrain_loss: 15.262458801269531\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1831\ttrain_loss: 15.47864055633545\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1832\ttrain_loss: 15.312677383422852\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1833\ttrain_loss: 15.324490547180176\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1834\ttrain_loss: 15.302511215209961\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1835\ttrain_loss: 15.203651428222656\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1836\ttrain_loss: 15.352291107177734\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1837\ttrain_loss: 15.328814506530762\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1838\ttrain_loss: 15.196381568908691\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1839\ttrain_loss: 15.103777885437012\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1840\ttrain_loss: 15.335131645202637\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1841\ttrain_loss: 15.206320762634277\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1842\ttrain_loss: 15.210809707641602\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1843\ttrain_loss: 15.357772827148438\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1844\ttrain_loss: 15.189108848571777\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1845\ttrain_loss: 15.270630836486816\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1846\ttrain_loss: 15.318967819213867\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1847\ttrain_loss: 15.36811351776123\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1848\ttrain_loss: 14.893815994262695\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1849\ttrain_loss: 15.234506607055664\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1850\ttrain_loss: 15.111747741699219\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1851\ttrain_loss: 15.24213695526123\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1852\ttrain_loss: 15.197823524475098\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1853\ttrain_loss: 15.2991304397583\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1854\ttrain_loss: 15.173005104064941\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1855\ttrain_loss: 15.360506057739258\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1856\ttrain_loss: 15.09291934967041\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1857\ttrain_loss: 15.279326438903809\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1858\ttrain_loss: 15.34839153289795\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1859\ttrain_loss: 15.282658576965332\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1860\ttrain_loss: 15.010258674621582\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1861\ttrain_loss: 15.213618278503418\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1862\ttrain_loss: 15.266203880310059\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1863\ttrain_loss: 15.412702560424805\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1864\ttrain_loss: 15.348719596862793\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1865\ttrain_loss: 15.395673751831055\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1866\ttrain_loss: 15.508539199829102\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1867\ttrain_loss: 15.072139739990234\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1868\ttrain_loss: 15.237655639648438\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1869\ttrain_loss: 15.4339017868042\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1870\ttrain_loss: 15.108436584472656\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1871\ttrain_loss: 15.033971786499023\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1872\ttrain_loss: 15.142037391662598\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1873\ttrain_loss: 15.269359588623047\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1874\ttrain_loss: 15.308780670166016\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1875\ttrain_loss: 15.290456771850586\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1876\ttrain_loss: 15.124894142150879\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1877\ttrain_loss: 15.06997013092041\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1878\ttrain_loss: 15.310456275939941\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1879\ttrain_loss: 15.223477363586426\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1880\ttrain_loss: 15.156225204467773\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1881\ttrain_loss: 15.225932121276855\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1882\ttrain_loss: 15.271585464477539\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1883\ttrain_loss: 15.357351303100586\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1884\ttrain_loss: 15.338074684143066\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1885\ttrain_loss: 15.163214683532715\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1886\ttrain_loss: 15.442048072814941\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1887\ttrain_loss: 15.329798698425293\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1888\ttrain_loss: 15.104926109313965\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1889\ttrain_loss: 15.093546867370605\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1890\ttrain_loss: 15.112204551696777\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1891\ttrain_loss: 15.307988166809082\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1892\ttrain_loss: 15.149537086486816\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1893\ttrain_loss: 15.395217895507812\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1894\ttrain_loss: 15.31795883178711\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1895\ttrain_loss: 15.298726081848145\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1896\ttrain_loss: 15.200181007385254\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1897\ttrain_loss: 15.272327423095703\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1898\ttrain_loss: 15.236918449401855\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1899\ttrain_loss: 15.11273193359375\tval_loss: 15.249105453491211\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1900\ttrain_loss: 15.287676811218262\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1901\ttrain_loss: 15.130841255187988\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1902\ttrain_loss: 15.19487190246582\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1903\ttrain_loss: 15.277846336364746\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1904\ttrain_loss: 15.240675926208496\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1905\ttrain_loss: 15.350728034973145\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1906\ttrain_loss: 15.286794662475586\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1907\ttrain_loss: 15.2202787399292\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1908\ttrain_loss: 15.528242111206055\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1909\ttrain_loss: 15.332192420959473\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1910\ttrain_loss: 15.198053359985352\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1911\ttrain_loss: 15.24050521850586\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1912\ttrain_loss: 15.448739051818848\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1913\ttrain_loss: 15.02017593383789\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1914\ttrain_loss: 15.39583969116211\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1915\ttrain_loss: 15.434011459350586\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1916\ttrain_loss: 15.117951393127441\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1917\ttrain_loss: 15.153092384338379\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1918\ttrain_loss: 15.32441520690918\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1919\ttrain_loss: 15.245245933532715\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1920\ttrain_loss: 15.182462692260742\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1921\ttrain_loss: 15.424471855163574\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1922\ttrain_loss: 15.225177764892578\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1923\ttrain_loss: 15.22000503540039\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1924\ttrain_loss: 15.155708312988281\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1925\ttrain_loss: 15.25853157043457\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1926\ttrain_loss: 15.309192657470703\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1927\ttrain_loss: 15.176207542419434\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1928\ttrain_loss: 15.183300018310547\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1929\ttrain_loss: 15.317150115966797\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1930\ttrain_loss: 15.142743110656738\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1931\ttrain_loss: 15.448155403137207\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1932\ttrain_loss: 15.289022445678711\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1933\ttrain_loss: 15.229239463806152\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1934\ttrain_loss: 15.156137466430664\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1935\ttrain_loss: 15.363761901855469\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1936\ttrain_loss: 15.12209415435791\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1937\ttrain_loss: 15.04121208190918\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1938\ttrain_loss: 15.079343795776367\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1939\ttrain_loss: 15.245782852172852\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1940\ttrain_loss: 15.07356071472168\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1941\ttrain_loss: 15.408156394958496\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1942\ttrain_loss: 15.15671157836914\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1943\ttrain_loss: 15.168413162231445\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1944\ttrain_loss: 15.40916919708252\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1945\ttrain_loss: 15.10786247253418\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1946\ttrain_loss: 15.26229190826416\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1947\ttrain_loss: 15.164759635925293\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1948\ttrain_loss: 15.06190013885498\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1949\ttrain_loss: 15.25076675415039\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1950\ttrain_loss: 15.196807861328125\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1951\ttrain_loss: 15.505955696105957\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1952\ttrain_loss: 15.531883239746094\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1953\ttrain_loss: 15.196367263793945\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1954\ttrain_loss: 15.13325309753418\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1955\ttrain_loss: 15.274679183959961\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1956\ttrain_loss: 15.300737380981445\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1957\ttrain_loss: 15.254463195800781\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1958\ttrain_loss: 15.287276268005371\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1959\ttrain_loss: 15.131998062133789\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1960\ttrain_loss: 15.088212966918945\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1961\ttrain_loss: 15.416011810302734\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1962\ttrain_loss: 15.29357624053955\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1963\ttrain_loss: 15.405393600463867\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1964\ttrain_loss: 15.021247863769531\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1965\ttrain_loss: 15.491535186767578\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1966\ttrain_loss: 15.068204879760742\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1967\ttrain_loss: 15.248112678527832\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1968\ttrain_loss: 15.45848560333252\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1969\ttrain_loss: 15.248517990112305\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1970\ttrain_loss: 15.292634963989258\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1971\ttrain_loss: 15.056447982788086\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1972\ttrain_loss: 15.015874862670898\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1973\ttrain_loss: 15.156913757324219\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1974\ttrain_loss: 15.185663223266602\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1975\ttrain_loss: 15.255992889404297\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1976\ttrain_loss: 15.173869132995605\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1977\ttrain_loss: 14.998559951782227\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1978\ttrain_loss: 15.29062271118164\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1979\ttrain_loss: 15.442299842834473\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1980\ttrain_loss: 15.437034606933594\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1981\ttrain_loss: 15.230243682861328\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1982\ttrain_loss: 15.294894218444824\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1983\ttrain_loss: 15.203849792480469\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1984\ttrain_loss: 15.314752578735352\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1985\ttrain_loss: 15.176131248474121\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1986\ttrain_loss: 15.169827461242676\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1987\ttrain_loss: 15.29779052734375\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1988\ttrain_loss: 15.0872163772583\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1989\ttrain_loss: 15.270669937133789\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1990\ttrain_loss: 15.298209190368652\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1991\ttrain_loss: 15.167708396911621\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1992\ttrain_loss: 15.20244312286377\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1993\ttrain_loss: 15.16740608215332\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1994\ttrain_loss: 15.25022029876709\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1995\ttrain_loss: 15.336101531982422\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1996\ttrain_loss: 15.267681121826172\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1997\ttrain_loss: 15.147590637207031\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1998\ttrain_loss: 15.36905574798584\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 1999\ttrain_loss: 15.395209312438965\tval_loss: 15.248306274414062\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2000\ttrain_loss: 15.283255577087402\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2001\ttrain_loss: 15.383842468261719\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2002\ttrain_loss: 15.238822937011719\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2003\ttrain_loss: 15.131534576416016\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2004\ttrain_loss: 15.217535018920898\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2005\ttrain_loss: 15.041744232177734\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2006\ttrain_loss: 15.131314277648926\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2007\ttrain_loss: 15.080668449401855\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2008\ttrain_loss: 15.318825721740723\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2009\ttrain_loss: 15.21267032623291\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2010\ttrain_loss: 15.192687034606934\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2011\ttrain_loss: 15.28611946105957\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2012\ttrain_loss: 15.179718017578125\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2013\ttrain_loss: 15.551928520202637\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2014\ttrain_loss: 15.094815254211426\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2015\ttrain_loss: 15.218788146972656\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2016\ttrain_loss: 15.25666618347168\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2017\ttrain_loss: 15.297492027282715\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2018\ttrain_loss: 15.197421073913574\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2019\ttrain_loss: 15.10394287109375\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2020\ttrain_loss: 15.392351150512695\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2021\ttrain_loss: 15.05418586730957\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2022\ttrain_loss: 15.3674955368042\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2023\ttrain_loss: 15.314393997192383\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2024\ttrain_loss: 15.293229103088379\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2025\ttrain_loss: 15.324568748474121\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2026\ttrain_loss: 15.24788761138916\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2027\ttrain_loss: 15.103721618652344\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2028\ttrain_loss: 15.302450180053711\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2029\ttrain_loss: 15.295217514038086\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2030\ttrain_loss: 15.369063377380371\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2031\ttrain_loss: 15.176050186157227\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2032\ttrain_loss: 15.285523414611816\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2033\ttrain_loss: 15.42038345336914\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2034\ttrain_loss: 15.261198043823242\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2035\ttrain_loss: 15.154745101928711\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2036\ttrain_loss: 15.224259376525879\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2037\ttrain_loss: 15.343212127685547\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2038\ttrain_loss: 15.177473068237305\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2039\ttrain_loss: 15.439067840576172\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2040\ttrain_loss: 15.384873390197754\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2041\ttrain_loss: 15.324176788330078\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2042\ttrain_loss: 15.18734359741211\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2043\ttrain_loss: 15.110687255859375\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2044\ttrain_loss: 15.299843788146973\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2045\ttrain_loss: 15.217513084411621\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2046\ttrain_loss: 15.2714204788208\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2047\ttrain_loss: 15.34982967376709\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2048\ttrain_loss: 15.247191429138184\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2049\ttrain_loss: 15.043743133544922\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2050\ttrain_loss: 15.276432037353516\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2051\ttrain_loss: 15.29321575164795\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2052\ttrain_loss: 15.047904014587402\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2053\ttrain_loss: 15.191385269165039\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2054\ttrain_loss: 15.242402076721191\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2055\ttrain_loss: 15.252143859863281\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2056\ttrain_loss: 15.212789535522461\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2057\ttrain_loss: 15.227214813232422\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2058\ttrain_loss: 15.283120155334473\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2059\ttrain_loss: 15.373553276062012\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2060\ttrain_loss: 15.314574241638184\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2061\ttrain_loss: 15.294251441955566\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2062\ttrain_loss: 14.980015754699707\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2063\ttrain_loss: 15.257791519165039\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2064\ttrain_loss: 15.250455856323242\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2065\ttrain_loss: 15.44571590423584\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2066\ttrain_loss: 15.324146270751953\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2067\ttrain_loss: 15.32706069946289\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2068\ttrain_loss: 15.11597728729248\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2069\ttrain_loss: 14.970504760742188\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2070\ttrain_loss: 15.30601692199707\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2071\ttrain_loss: 15.17383098602295\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2072\ttrain_loss: 15.083738327026367\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2073\ttrain_loss: 15.074474334716797\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2074\ttrain_loss: 15.390503883361816\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2075\ttrain_loss: 15.27469253540039\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2076\ttrain_loss: 15.309908866882324\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2077\ttrain_loss: 15.302157402038574\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2078\ttrain_loss: 15.166487693786621\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2079\ttrain_loss: 15.355579376220703\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2080\ttrain_loss: 14.97397518157959\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2081\ttrain_loss: 15.141142845153809\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2082\ttrain_loss: 15.36793041229248\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2083\ttrain_loss: 15.274611473083496\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2084\ttrain_loss: 15.163174629211426\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2085\ttrain_loss: 15.25539493560791\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2086\ttrain_loss: 15.183704376220703\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2087\ttrain_loss: 15.31587028503418\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2088\ttrain_loss: 15.11780071258545\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2089\ttrain_loss: 15.316606521606445\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2090\ttrain_loss: 15.197710037231445\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2091\ttrain_loss: 15.120229721069336\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2092\ttrain_loss: 15.419609069824219\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2093\ttrain_loss: 15.293706893920898\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2094\ttrain_loss: 15.370229721069336\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2095\ttrain_loss: 15.21281623840332\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2096\ttrain_loss: 15.171808242797852\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2097\ttrain_loss: 15.349536895751953\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2098\ttrain_loss: 15.320096015930176\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2099\ttrain_loss: 15.288538932800293\tval_loss: 15.24732780456543\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 2100\ttrain_loss: 15.16545581817627\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2101\ttrain_loss: 15.228400230407715\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2102\ttrain_loss: 15.577706336975098\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2103\ttrain_loss: 15.238385200500488\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2104\ttrain_loss: 15.156496047973633\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2105\ttrain_loss: 15.208788871765137\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2106\ttrain_loss: 15.361629486083984\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2107\ttrain_loss: 15.300322532653809\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2108\ttrain_loss: 15.229127883911133\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2109\ttrain_loss: 15.269582748413086\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2110\ttrain_loss: 15.333971977233887\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2111\ttrain_loss: 15.089442253112793\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2112\ttrain_loss: 15.102187156677246\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2113\ttrain_loss: 15.273648262023926\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2114\ttrain_loss: 15.152828216552734\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2115\ttrain_loss: 15.4575834274292\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2116\ttrain_loss: 15.174276351928711\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2117\ttrain_loss: 15.330705642700195\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2118\ttrain_loss: 15.09270191192627\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2119\ttrain_loss: 15.132905006408691\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2120\ttrain_loss: 15.275426864624023\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2121\ttrain_loss: 15.120049476623535\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2122\ttrain_loss: 15.202427864074707\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2123\ttrain_loss: 15.261103630065918\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2124\ttrain_loss: 15.358631134033203\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2125\ttrain_loss: 15.199185371398926\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2126\ttrain_loss: 15.149304389953613\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2127\ttrain_loss: 15.24222183227539\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2128\ttrain_loss: 15.26500415802002\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2129\ttrain_loss: 15.163378715515137\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2130\ttrain_loss: 15.18887996673584\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2131\ttrain_loss: 15.21419906616211\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2132\ttrain_loss: 15.365609169006348\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2133\ttrain_loss: 15.482184410095215\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2134\ttrain_loss: 15.32955265045166\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2135\ttrain_loss: 15.429659843444824\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2136\ttrain_loss: 15.33443832397461\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2137\ttrain_loss: 15.42358112335205\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2138\ttrain_loss: 15.281949043273926\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2139\ttrain_loss: 15.285008430480957\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2140\ttrain_loss: 15.200664520263672\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2141\ttrain_loss: 15.200139999389648\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2142\ttrain_loss: 15.4124174118042\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2143\ttrain_loss: 15.201708793640137\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2144\ttrain_loss: 15.092456817626953\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2145\ttrain_loss: 15.149918556213379\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2146\ttrain_loss: 15.309584617614746\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2147\ttrain_loss: 15.394972801208496\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2148\ttrain_loss: 15.170245170593262\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2149\ttrain_loss: 15.263134956359863\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2150\ttrain_loss: 15.13578987121582\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2151\ttrain_loss: 15.264744758605957\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2152\ttrain_loss: 15.067938804626465\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2153\ttrain_loss: 15.312941551208496\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2154\ttrain_loss: 15.405582427978516\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2155\ttrain_loss: 15.339678764343262\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2156\ttrain_loss: 14.981904983520508\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2157\ttrain_loss: 15.187615394592285\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2158\ttrain_loss: 15.108664512634277\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2159\ttrain_loss: 15.155891418457031\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2160\ttrain_loss: 15.06763744354248\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2161\ttrain_loss: 15.402966499328613\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2162\ttrain_loss: 15.109363555908203\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2163\ttrain_loss: 15.19771957397461\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2164\ttrain_loss: 15.292953491210938\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2165\ttrain_loss: 15.319087028503418\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2166\ttrain_loss: 15.360468864440918\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2167\ttrain_loss: 15.203307151794434\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2168\ttrain_loss: 15.042693138122559\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2169\ttrain_loss: 15.222590446472168\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2170\ttrain_loss: 15.2113618850708\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2171\ttrain_loss: 15.279631614685059\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2172\ttrain_loss: 15.405008316040039\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2173\ttrain_loss: 15.326740264892578\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2174\ttrain_loss: 15.184988021850586\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2175\ttrain_loss: 15.38416862487793\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2176\ttrain_loss: 15.350425720214844\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2177\ttrain_loss: 15.478384971618652\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2178\ttrain_loss: 15.398918151855469\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2179\ttrain_loss: 15.388960838317871\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2180\ttrain_loss: 15.127979278564453\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2181\ttrain_loss: 15.160863876342773\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2182\ttrain_loss: 15.082348823547363\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2183\ttrain_loss: 15.271186828613281\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2184\ttrain_loss: 15.388432502746582\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2185\ttrain_loss: 15.409528732299805\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2186\ttrain_loss: 15.381431579589844\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2187\ttrain_loss: 15.419313430786133\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2188\ttrain_loss: 15.185747146606445\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2189\ttrain_loss: 15.155656814575195\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2190\ttrain_loss: 15.211294174194336\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2191\ttrain_loss: 15.124971389770508\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2192\ttrain_loss: 15.006742477416992\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2193\ttrain_loss: 15.314435958862305\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2194\ttrain_loss: 15.284158706665039\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2195\ttrain_loss: 15.160551071166992\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2196\ttrain_loss: 15.242145538330078\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2197\ttrain_loss: 15.28818130493164\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2198\ttrain_loss: 15.309708595275879\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2199\ttrain_loss: 15.284955024719238\tval_loss: 15.247018814086914\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2200\ttrain_loss: 15.31625747680664\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2201\ttrain_loss: 15.267887115478516\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2202\ttrain_loss: 15.177066802978516\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2203\ttrain_loss: 15.284994125366211\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2204\ttrain_loss: 15.20538330078125\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2205\ttrain_loss: 15.44040298461914\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2206\ttrain_loss: 15.22031307220459\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2207\ttrain_loss: 15.30360221862793\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2208\ttrain_loss: 15.12845516204834\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2209\ttrain_loss: 15.244772911071777\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2210\ttrain_loss: 15.333348274230957\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2211\ttrain_loss: 15.20491886138916\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2212\ttrain_loss: 15.232425689697266\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 2213\ttrain_loss: 15.311359405517578\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2214\ttrain_loss: 15.15578556060791\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2215\ttrain_loss: 15.331488609313965\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2216\ttrain_loss: 15.135345458984375\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2217\ttrain_loss: 15.2731294631958\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2218\ttrain_loss: 15.168652534484863\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2219\ttrain_loss: 15.337089538574219\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2220\ttrain_loss: 15.310321807861328\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2221\ttrain_loss: 15.11514949798584\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2222\ttrain_loss: 15.12086009979248\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2223\ttrain_loss: 15.270731925964355\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2224\ttrain_loss: 15.145343780517578\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2225\ttrain_loss: 14.879374504089355\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2226\ttrain_loss: 15.306228637695312\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2227\ttrain_loss: 15.401586532592773\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2228\ttrain_loss: 15.17883586883545\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2229\ttrain_loss: 15.0093355178833\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2230\ttrain_loss: 15.101081848144531\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2231\ttrain_loss: 15.288948059082031\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2232\ttrain_loss: 15.292475700378418\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2233\ttrain_loss: 15.234235763549805\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2234\ttrain_loss: 15.164730072021484\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2235\ttrain_loss: 15.176490783691406\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2236\ttrain_loss: 15.51964282989502\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2237\ttrain_loss: 15.151506423950195\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2238\ttrain_loss: 15.2847318649292\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2239\ttrain_loss: 15.360713958740234\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2240\ttrain_loss: 15.489410400390625\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2241\ttrain_loss: 15.215603828430176\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2242\ttrain_loss: 15.409296989440918\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2243\ttrain_loss: 15.10171890258789\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2244\ttrain_loss: 15.09914779663086\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2245\ttrain_loss: 15.228857040405273\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2246\ttrain_loss: 15.129490852355957\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2247\ttrain_loss: 15.413093566894531\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2248\ttrain_loss: 15.27097225189209\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2249\ttrain_loss: 15.263162612915039\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2250\ttrain_loss: 15.347016334533691\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2251\ttrain_loss: 15.188690185546875\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2252\ttrain_loss: 15.189231872558594\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2253\ttrain_loss: 15.326777458190918\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2254\ttrain_loss: 15.178922653198242\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2255\ttrain_loss: 15.171745300292969\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2256\ttrain_loss: 15.213569641113281\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2257\ttrain_loss: 15.361966133117676\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2258\ttrain_loss: 15.215514183044434\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2259\ttrain_loss: 15.236709594726562\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2260\ttrain_loss: 15.184810638427734\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2261\ttrain_loss: 15.21341609954834\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2262\ttrain_loss: 15.264358520507812\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2263\ttrain_loss: 15.230751991271973\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2264\ttrain_loss: 15.336027145385742\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2265\ttrain_loss: 15.307456016540527\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2266\ttrain_loss: 15.204239845275879\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2267\ttrain_loss: 15.140878677368164\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2268\ttrain_loss: 15.107569694519043\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2269\ttrain_loss: 15.185094833374023\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2270\ttrain_loss: 15.24540901184082\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2271\ttrain_loss: 15.298776626586914\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2272\ttrain_loss: 15.347883224487305\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2273\ttrain_loss: 15.360803604125977\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2274\ttrain_loss: 15.411115646362305\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2275\ttrain_loss: 15.25163459777832\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2276\ttrain_loss: 15.196969985961914\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2277\ttrain_loss: 15.12651252746582\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2278\ttrain_loss: 15.038655281066895\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2279\ttrain_loss: 15.528339385986328\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2280\ttrain_loss: 15.245654106140137\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2281\ttrain_loss: 15.088549613952637\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2282\ttrain_loss: 15.52029800415039\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2283\ttrain_loss: 15.180147171020508\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2284\ttrain_loss: 15.187764167785645\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2285\ttrain_loss: 15.256396293640137\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2286\ttrain_loss: 15.04389476776123\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2287\ttrain_loss: 15.162924766540527\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2288\ttrain_loss: 15.325531005859375\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2289\ttrain_loss: 15.197932243347168\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2290\ttrain_loss: 15.266338348388672\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2291\ttrain_loss: 15.21762752532959\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2292\ttrain_loss: 15.18674087524414\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2293\ttrain_loss: 15.350662231445312\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2294\ttrain_loss: 15.068687438964844\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2295\ttrain_loss: 15.282804489135742\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2296\ttrain_loss: 15.311492919921875\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2297\ttrain_loss: 15.219605445861816\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2298\ttrain_loss: 15.415624618530273\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2299\ttrain_loss: 15.154562950134277\tval_loss: 15.246428489685059\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2300\ttrain_loss: 15.207265853881836\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2301\ttrain_loss: 15.209254264831543\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2302\ttrain_loss: 15.249580383300781\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2303\ttrain_loss: 15.420770645141602\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2304\ttrain_loss: 15.318379402160645\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2305\ttrain_loss: 15.270856857299805\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2306\ttrain_loss: 15.208832740783691\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2307\ttrain_loss: 15.030745506286621\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2308\ttrain_loss: 15.29434585571289\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2309\ttrain_loss: 15.172563552856445\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2310\ttrain_loss: 15.324603080749512\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2311\ttrain_loss: 15.071441650390625\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2312\ttrain_loss: 15.229460716247559\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2313\ttrain_loss: 15.04576587677002\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2314\ttrain_loss: 15.115763664245605\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2315\ttrain_loss: 15.283642768859863\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2316\ttrain_loss: 15.164700508117676\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2317\ttrain_loss: 15.312941551208496\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2318\ttrain_loss: 15.399497032165527\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2319\ttrain_loss: 15.212576866149902\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2320\ttrain_loss: 15.297538757324219\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2321\ttrain_loss: 15.057862281799316\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2322\ttrain_loss: 15.194777488708496\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2323\ttrain_loss: 15.254659652709961\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2324\ttrain_loss: 15.065228462219238\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2325\ttrain_loss: 15.20041561126709\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2326\ttrain_loss: 15.328947067260742\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2327\ttrain_loss: 15.393572807312012\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2328\ttrain_loss: 15.305682182312012\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 2329\ttrain_loss: 15.329121589660645\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2330\ttrain_loss: 15.252707481384277\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2331\ttrain_loss: 15.137494087219238\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2332\ttrain_loss: 15.101861953735352\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2333\ttrain_loss: 15.018357276916504\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2334\ttrain_loss: 15.037883758544922\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2335\ttrain_loss: 15.24647045135498\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2336\ttrain_loss: 15.275690078735352\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2337\ttrain_loss: 15.295517921447754\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2338\ttrain_loss: 15.631367683410645\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2339\ttrain_loss: 15.230599403381348\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2340\ttrain_loss: 15.310548782348633\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2341\ttrain_loss: 15.418800354003906\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2342\ttrain_loss: 15.220322608947754\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2343\ttrain_loss: 15.095263481140137\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2344\ttrain_loss: 15.281981468200684\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2345\ttrain_loss: 15.250307083129883\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2346\ttrain_loss: 15.07298469543457\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2347\ttrain_loss: 15.096179962158203\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2348\ttrain_loss: 15.42828369140625\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2349\ttrain_loss: 15.237772941589355\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2350\ttrain_loss: 15.590668678283691\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2351\ttrain_loss: 15.086712837219238\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2352\ttrain_loss: 15.120827674865723\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2353\ttrain_loss: 15.293940544128418\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2354\ttrain_loss: 15.302809715270996\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2355\ttrain_loss: 15.183191299438477\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2356\ttrain_loss: 15.095863342285156\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2357\ttrain_loss: 15.379388809204102\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2358\ttrain_loss: 15.305641174316406\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2359\ttrain_loss: 15.300263404846191\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2360\ttrain_loss: 15.263833045959473\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2361\ttrain_loss: 15.218854904174805\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2362\ttrain_loss: 15.38096809387207\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2363\ttrain_loss: 15.249481201171875\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2364\ttrain_loss: 15.235260009765625\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2365\ttrain_loss: 15.283711433410645\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2366\ttrain_loss: 15.272178649902344\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2367\ttrain_loss: 15.213652610778809\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2368\ttrain_loss: 15.124167442321777\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2369\ttrain_loss: 15.15971851348877\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2370\ttrain_loss: 15.175881385803223\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2371\ttrain_loss: 15.321168899536133\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2372\ttrain_loss: 15.32534122467041\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2373\ttrain_loss: 15.341361045837402\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2374\ttrain_loss: 15.254810333251953\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2375\ttrain_loss: 15.338590621948242\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2376\ttrain_loss: 15.214025497436523\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2377\ttrain_loss: 15.218795776367188\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2378\ttrain_loss: 15.271698951721191\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2379\ttrain_loss: 15.543170928955078\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2380\ttrain_loss: 15.213314056396484\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2381\ttrain_loss: 15.101500511169434\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2382\ttrain_loss: 15.017289161682129\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2383\ttrain_loss: 15.223217964172363\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2384\ttrain_loss: 15.234346389770508\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2385\ttrain_loss: 15.438865661621094\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2386\ttrain_loss: 15.309603691101074\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2387\ttrain_loss: 15.384818077087402\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2388\ttrain_loss: 15.353074073791504\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2389\ttrain_loss: 15.156523704528809\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2390\ttrain_loss: 15.256150245666504\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2391\ttrain_loss: 15.227495193481445\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2392\ttrain_loss: 15.176416397094727\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2393\ttrain_loss: 15.288484573364258\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2394\ttrain_loss: 15.161547660827637\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2395\ttrain_loss: 15.196111679077148\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2396\ttrain_loss: 15.325155258178711\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2397\ttrain_loss: 15.42757511138916\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2398\ttrain_loss: 15.236185073852539\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2399\ttrain_loss: 15.248010635375977\tval_loss: 15.245453834533691\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2400\ttrain_loss: 15.175769805908203\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2401\ttrain_loss: 15.02033805847168\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2402\ttrain_loss: 15.295513153076172\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2403\ttrain_loss: 15.232198715209961\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2404\ttrain_loss: 15.264153480529785\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2405\ttrain_loss: 15.092811584472656\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2406\ttrain_loss: 15.244404792785645\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2407\ttrain_loss: 15.203415870666504\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2408\ttrain_loss: 15.17431354522705\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2409\ttrain_loss: 15.36614990234375\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2410\ttrain_loss: 15.17352294921875\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2411\ttrain_loss: 15.270689964294434\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2412\ttrain_loss: 15.22057819366455\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2413\ttrain_loss: 15.232848167419434\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2414\ttrain_loss: 15.342504501342773\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2415\ttrain_loss: 15.05139446258545\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2416\ttrain_loss: 15.527584075927734\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2417\ttrain_loss: 15.150060653686523\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2418\ttrain_loss: 15.132853507995605\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2419\ttrain_loss: 15.304229736328125\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2420\ttrain_loss: 15.235915184020996\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2421\ttrain_loss: 15.316962242126465\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2422\ttrain_loss: 15.27286148071289\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2423\ttrain_loss: 15.280790328979492\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2424\ttrain_loss: 15.475166320800781\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2425\ttrain_loss: 15.138738632202148\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2426\ttrain_loss: 15.273714065551758\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2427\ttrain_loss: 15.200674057006836\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2428\ttrain_loss: 15.220439910888672\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2429\ttrain_loss: 15.107891082763672\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2430\ttrain_loss: 15.278291702270508\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2431\ttrain_loss: 15.371634483337402\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2432\ttrain_loss: 15.400218963623047\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2433\ttrain_loss: 15.123734474182129\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2434\ttrain_loss: 15.252642631530762\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2435\ttrain_loss: 15.275279998779297\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2436\ttrain_loss: 15.33552074432373\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2437\ttrain_loss: 15.133307456970215\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2438\ttrain_loss: 15.248655319213867\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2439\ttrain_loss: 15.166874885559082\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2440\ttrain_loss: 15.180988311767578\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2441\ttrain_loss: 15.187346458435059\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2442\ttrain_loss: 15.212905883789062\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2443\ttrain_loss: 15.34679889678955\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2444\ttrain_loss: 15.189789772033691\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2445\ttrain_loss: 15.011270523071289\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2446\ttrain_loss: 15.219882011413574\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 2447\ttrain_loss: 15.338003158569336\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2448\ttrain_loss: 15.223417282104492\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2449\ttrain_loss: 15.29391860961914\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2450\ttrain_loss: 15.22107219696045\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2451\ttrain_loss: 15.179549217224121\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2452\ttrain_loss: 15.10671615600586\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2453\ttrain_loss: 15.382514953613281\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2454\ttrain_loss: 15.408220291137695\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2455\ttrain_loss: 15.148228645324707\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2456\ttrain_loss: 15.269227981567383\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2457\ttrain_loss: 15.20789623260498\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2458\ttrain_loss: 15.413349151611328\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2459\ttrain_loss: 15.216814041137695\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2460\ttrain_loss: 15.307884216308594\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2461\ttrain_loss: 15.15100383758545\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2462\ttrain_loss: 15.335638999938965\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2463\ttrain_loss: 15.212018966674805\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2464\ttrain_loss: 15.190942764282227\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2465\ttrain_loss: 15.414490699768066\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2466\ttrain_loss: 15.206250190734863\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2467\ttrain_loss: 15.48373794555664\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2468\ttrain_loss: 15.130145072937012\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2469\ttrain_loss: 15.101118087768555\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2470\ttrain_loss: 15.193470001220703\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2471\ttrain_loss: 15.285513877868652\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2472\ttrain_loss: 15.092690467834473\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2473\ttrain_loss: 15.104866027832031\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2474\ttrain_loss: 15.290002822875977\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2475\ttrain_loss: 15.139808654785156\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2476\ttrain_loss: 15.260964393615723\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2477\ttrain_loss: 15.145759582519531\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2478\ttrain_loss: 15.252588272094727\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2479\ttrain_loss: 15.074570655822754\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2480\ttrain_loss: 15.269806861877441\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2481\ttrain_loss: 15.243367195129395\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2482\ttrain_loss: 15.225395202636719\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2483\ttrain_loss: 15.063055038452148\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2484\ttrain_loss: 15.241708755493164\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2485\ttrain_loss: 15.154580116271973\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2486\ttrain_loss: 15.37884521484375\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2487\ttrain_loss: 15.327767372131348\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2488\ttrain_loss: 15.29167652130127\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2489\ttrain_loss: 15.269290924072266\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2490\ttrain_loss: 15.154084205627441\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2491\ttrain_loss: 15.278239250183105\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2492\ttrain_loss: 15.141542434692383\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2493\ttrain_loss: 15.265313148498535\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2494\ttrain_loss: 15.25401496887207\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2495\ttrain_loss: 15.54824447631836\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2496\ttrain_loss: 15.293878555297852\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2497\ttrain_loss: 15.17515754699707\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2498\ttrain_loss: 15.187732696533203\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2499\ttrain_loss: 15.294431686401367\tval_loss: 15.244722366333008\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2500\ttrain_loss: 15.111822128295898\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2501\ttrain_loss: 15.273818969726562\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2502\ttrain_loss: 15.42134952545166\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2503\ttrain_loss: 15.016280174255371\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2504\ttrain_loss: 15.340229034423828\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2505\ttrain_loss: 15.361695289611816\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2506\ttrain_loss: 15.063960075378418\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2507\ttrain_loss: 15.307790756225586\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2508\ttrain_loss: 15.307884216308594\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2509\ttrain_loss: 15.188360214233398\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2510\ttrain_loss: 15.142664909362793\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2511\ttrain_loss: 15.270245552062988\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2512\ttrain_loss: 15.471221923828125\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2513\ttrain_loss: 15.035000801086426\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2514\ttrain_loss: 15.209484100341797\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2515\ttrain_loss: 15.393280982971191\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2516\ttrain_loss: 14.988431930541992\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2517\ttrain_loss: 15.360367774963379\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2518\ttrain_loss: 15.351968765258789\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2519\ttrain_loss: 15.210317611694336\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2520\ttrain_loss: 15.073680877685547\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2521\ttrain_loss: 15.160388946533203\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2522\ttrain_loss: 15.112793922424316\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2523\ttrain_loss: 15.112192153930664\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2524\ttrain_loss: 15.205582618713379\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2525\ttrain_loss: 15.193324089050293\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2526\ttrain_loss: 15.058318138122559\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2527\ttrain_loss: 15.155950546264648\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2528\ttrain_loss: 15.192431449890137\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2529\ttrain_loss: 15.024100303649902\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2530\ttrain_loss: 15.233074188232422\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2531\ttrain_loss: 15.367374420166016\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2532\ttrain_loss: 15.294744491577148\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2533\ttrain_loss: 15.296399116516113\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2534\ttrain_loss: 15.414633750915527\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2535\ttrain_loss: 15.156749725341797\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2536\ttrain_loss: 15.364236831665039\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2537\ttrain_loss: 15.159406661987305\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2538\ttrain_loss: 15.171416282653809\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2539\ttrain_loss: 15.119162559509277\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2540\ttrain_loss: 15.216201782226562\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2541\ttrain_loss: 15.298213958740234\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2542\ttrain_loss: 15.164125442504883\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2543\ttrain_loss: 15.330324172973633\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2544\ttrain_loss: 15.338119506835938\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2545\ttrain_loss: 15.202051162719727\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2546\ttrain_loss: 15.294495582580566\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2547\ttrain_loss: 15.273619651794434\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2548\ttrain_loss: 15.36803913116455\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2549\ttrain_loss: 15.280519485473633\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2550\ttrain_loss: 15.061858177185059\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2551\ttrain_loss: 15.239130973815918\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2552\ttrain_loss: 15.324357032775879\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2553\ttrain_loss: 15.089998245239258\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2554\ttrain_loss: 15.285171508789062\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2555\ttrain_loss: 15.133376121520996\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2556\ttrain_loss: 15.281811714172363\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2557\ttrain_loss: 14.999253273010254\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2558\ttrain_loss: 15.47901439666748\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2559\ttrain_loss: 15.060308456420898\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2560\ttrain_loss: 15.092371940612793\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2561\ttrain_loss: 15.13059139251709\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2562\ttrain_loss: 14.985102653503418\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2563\ttrain_loss: 15.272029876708984\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 2564\ttrain_loss: 15.346292495727539\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2565\ttrain_loss: 15.542759895324707\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2566\ttrain_loss: 15.433966636657715\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2567\ttrain_loss: 15.25141429901123\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2568\ttrain_loss: 15.265914916992188\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2569\ttrain_loss: 15.188860893249512\tval_loss: 15.243876457214355\n",
      "(10,)\n",
      "\n",
      "\r",
      "Step 2570\ttrain_loss: 15.25406551361084\tval_loss: 15.243876457214355"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-6de7f8220eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     _, train_loss, p_x2_x1_tmp = sess.run([train_step, loss, p_x2_x1],\n\u001b[1;32m     10\u001b[0m                             feed_dict={input_data: sample_batch,\n\u001b[0;32m---> 11\u001b[0;31m                                       training: True})    \n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/virtual_env/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/virtual_env/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/virtual_env/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \"\"\"\n\u001b[1;32m   1309\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m     \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m     \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/virtual_env/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \"\"\"\n\u001b[1;32m   1309\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m     \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m     \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "for i in range(40001):\n",
    "    sample_batch = np.random.choice(len(train), size=batch_size)\n",
    "    sample_batch = train[sample_batch]\n",
    "    _, train_loss, p_x2_x1_tmp = sess.run([train_step, loss, p_x2_x1],\n",
    "                            feed_dict={input_data: sample_batch,\n",
    "                                      training: True})    \n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(p_x2_x1_tmp.shape)\n",
    "    print(\"\")\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        val_losses.append(np.mean(sess.run(loss, {input_data: val,\n",
    "                                                 training: False})))\n",
    "    \n",
    "    print('\\rStep {}\\ttrain_loss: {}\\tval_loss: {}'.format(i,\n",
    "                                                           train_losses[-1],\n",
    "                                                           val_losses[-1]), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(train_losses)), train_losses, label='train')\n",
    "plt.plot(np.arange(0, len(val_losses) * 100, 100), val_losses, label='val')\n",
    "plt.title('Training')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Negative log loss (bits)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datapoints = [(x, y) for x in range(200) for y in range(200)]\n",
    "final_ps = sess.run(final_prob, {input_data: all_datapoints, training: False}) \\\n",
    "    .reshape((200, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_p_x2_x1 = sess.run(p_x2_x1, {input_data: all_datapoints, training: False}) \\\n",
    "    .reshape((200, 200))\n",
    "\n",
    "final_p_x1 = sess.run(p_x1, {input_data: all_datapoints, training: False}) \\\n",
    "    .reshape((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 200 into shape (200,200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a64cbad2edbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfinal_ps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mall_datapoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 200 into shape (200,200)"
     ]
    }
   ],
   "source": [
    "# p_x1 = tf.gather(ps, x1s)\n",
    "\n",
    "final_ps = sess.run(ps, {input_data: all_datapoints, training: False}) \\\n",
    "    .reshape((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_p_x2_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_p_x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9958317e-05 1.8875893e-05 2.2873155e-05 1.7523960e-05 1.8030360e-05\n",
      " 1.1462737e-05 2.3208453e-05 1.7859473e-05 1.9479607e-05 2.1443864e-05\n",
      " 2.0357664e-05 1.6710226e-05 2.1241898e-05 2.4566449e-05 1.6528595e-05\n",
      " 2.0132687e-05 2.0837913e-05 1.5034740e-05 1.9615498e-05 1.9660009e-05\n",
      " 1.5874466e-05 1.7587217e-05 2.2155129e-05 1.5613057e-05 1.7922046e-05\n",
      " 2.1641896e-05 1.7913104e-05 1.9322726e-05 2.0847956e-05 1.7509128e-05\n",
      " 2.1930631e-05 2.1332178e-05 1.4406248e-05 1.8951589e-05 1.8672064e-05\n",
      " 1.7444638e-05 1.7001776e-05 2.3245193e-05 1.9598743e-05 2.0307665e-05\n",
      " 1.9085282e-05 1.9458130e-05 2.0820920e-05 1.9427829e-05 1.9202711e-05\n",
      " 1.7860488e-05 2.5035699e-05 1.8610905e-05 2.3808578e-05 2.2790435e-05\n",
      " 2.5498546e-05 2.1626664e-05 2.3378605e-05 1.7630018e-05 2.0081407e-05\n",
      " 2.0871104e-05 2.1848802e-05 1.9542656e-05 2.0146827e-05 1.8465562e-05\n",
      " 2.1406862e-05 2.3093568e-05 2.3858824e-05 2.1723479e-05 2.3569646e-05\n",
      " 2.1330621e-05 2.4987445e-05 2.3591541e-05 2.0726666e-05 2.4166833e-05\n",
      " 2.6348027e-05 1.7301103e-05 2.6419440e-05 2.5479378e-05 2.1972523e-05\n",
      " 2.9801224e-05 2.2126735e-05 2.3485814e-05 1.8773486e-05 2.6842081e-05\n",
      " 2.4583700e-05 1.9648347e-05 2.4891482e-05 2.5305946e-05 2.2046635e-05\n",
      " 2.0735388e-05 2.3758223e-05 1.7206476e-05 2.1169526e-05 2.2505097e-05\n",
      " 2.0528185e-05 2.3046558e-05 2.0738853e-05 2.3136276e-05 2.0967271e-05\n",
      " 2.1924705e-05 2.4632165e-05 2.2711107e-05 1.8381208e-05 2.0684187e-05\n",
      " 2.3212493e-05 2.1698273e-05 2.1200367e-05 2.7153159e-05 2.2144330e-05\n",
      " 2.3954688e-05 2.4156712e-05 2.0740286e-05 2.4049210e-05 2.6458565e-05\n",
      " 2.2719281e-05 2.6201660e-05 2.4178416e-05 2.7919044e-05 2.4924770e-05\n",
      " 2.5272664e-05 2.4824514e-05 2.7379258e-05 2.5130541e-05 2.4014576e-05\n",
      " 2.5337407e-05 2.3358940e-05 2.4176139e-05 2.5207841e-05 3.1415137e-05\n",
      " 2.7020324e-05 2.3032986e-05 2.5618345e-05 2.3829520e-05 2.2625887e-05\n",
      " 2.5248397e-05 2.4056293e-05 2.6332011e-05 2.4715062e-05 2.2974744e-05\n",
      " 2.1742802e-05 2.4622397e-05 2.5119123e-05 2.2243634e-05 1.8204497e-05\n",
      " 2.4008408e-05 2.4421573e-05 1.9414252e-05 2.0657557e-05 2.2813352e-05\n",
      " 1.9926581e-05 2.5380292e-05 2.2557710e-05 2.1919474e-05 2.7701104e-05\n",
      " 2.4160081e-05 2.1258251e-05 2.0221543e-05 2.3744282e-05 2.2755810e-05\n",
      " 1.9281199e-05 2.9547644e-05 2.0933820e-05 2.3126044e-05 2.3208742e-05\n",
      " 2.2162052e-05 2.3427570e-05 2.3781999e-05 2.6497120e-05 2.3890359e-05\n",
      " 2.1558986e-05 2.2271326e-05 2.2828599e-05 2.2095790e-05 2.1491840e-05\n",
      " 2.4754238e-05 2.0473904e-05 1.8750494e-05 2.2470291e-05 2.4606108e-05\n",
      " 2.4853451e-05 1.3233177e-05 1.9570356e-05 1.9705174e-05 1.8282753e-05\n",
      " 1.6254024e-05 1.7692992e-05 2.2956991e-05 2.1348324e-05 1.9958405e-05\n",
      " 1.5516564e-05 1.8729697e-05 1.9327845e-05 1.7203856e-05 2.0002499e-05\n",
      " 1.6525893e-05 1.6912503e-05 1.8136872e-05 2.4061528e-05 1.5992242e-05\n",
      " 2.0259249e-05 1.5761878e-05 1.6867998e-05 1.7928614e-05 1.6273551e-05]\n"
     ]
    }
   ],
   "source": [
    "print(final_ps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00461876 0.00436826 0.00529331 0.0040554  0.00417259 0.00265271\n",
      " 0.0053709  0.00413304 0.00450797 0.00496254 0.00471117 0.00386708\n",
      " 0.0049158  0.00568517 0.00382505 0.00465911 0.00482231 0.00347934\n",
      " 0.00453942 0.00454972 0.00367367 0.00407004 0.00512714 0.00361318\n",
      " 0.00414752 0.00500837 0.00414545 0.00447167 0.00482464 0.00405196\n",
      " 0.00507519 0.00493669 0.0033339  0.00438578 0.00432109 0.00403704\n",
      " 0.00393455 0.0053794  0.00453554 0.0046996  0.00441672 0.004503\n",
      " 0.00481838 0.00449599 0.00444389 0.00413328 0.00579376 0.00430694\n",
      " 0.00550978 0.00527416 0.00590088 0.00500484 0.00541028 0.00407994\n",
      " 0.00464724 0.00482999 0.00505625 0.00452256 0.00466238 0.0042733\n",
      " 0.00495398 0.00534432 0.00552141 0.00502725 0.00545449 0.00493633\n",
      " 0.0057826  0.00545956 0.00479657 0.00559269 0.00609746 0.00400382\n",
      " 0.00611399 0.00589644 0.00508488 0.0068966  0.00512057 0.00543509\n",
      " 0.00434456 0.0062118  0.00568916 0.00454702 0.00576039 0.0058563\n",
      " 0.00510203 0.00479859 0.00549813 0.00398192 0.00489905 0.00520813\n",
      " 0.00475063 0.00533344 0.00479939 0.0053542  0.00485225 0.00507382\n",
      " 0.00570038 0.00525581 0.00425378 0.00478674 0.00537184 0.00502142\n",
      " 0.00490619 0.00628379 0.00512464 0.0055436  0.00559035 0.00479972\n",
      " 0.00556547 0.00612304 0.0052577  0.00606359 0.00559537 0.00646103\n",
      " 0.00576809 0.0058486  0.00574489 0.00633611 0.00581571 0.00555746\n",
      " 0.00586359 0.00540573 0.00559484 0.0058336  0.00727009 0.00625305\n",
      " 0.0053303  0.0059286  0.00551463 0.00523609 0.00584299 0.00556711\n",
      " 0.00609376 0.00571956 0.00531682 0.00503172 0.00569812 0.00581307\n",
      " 0.00514762 0.00421289 0.00555603 0.00565164 0.00449285 0.00478057\n",
      " 0.00527947 0.00461141 0.00587351 0.00522031 0.00507261 0.00641059\n",
      " 0.00559113 0.00491959 0.00467967 0.0054949  0.00526615 0.00446206\n",
      " 0.00683792 0.00484451 0.00535183 0.00537097 0.00512874 0.00542161\n",
      " 0.00550363 0.00613197 0.00552871 0.00498918 0.00515403 0.005283\n",
      " 0.00511341 0.00497364 0.00572863 0.00473807 0.00433924 0.00520008\n",
      " 0.00569435 0.00575159 0.00306242 0.00452897 0.00456017 0.004231\n",
      " 0.00376151 0.00409451 0.00531271 0.00494043 0.00461878 0.00359084\n",
      " 0.00433443 0.00447285 0.00398132 0.00462898 0.00382442 0.00391389\n",
      " 0.00419724 0.00556832 0.00370093 0.0046884  0.00364762 0.00390359\n",
      " 0.00414904 0.00376603]\n"
     ]
    }
   ],
   "source": [
    "print(final_p_x2_x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        self.register_buffer('mask', torch.ones([out_features, in_features]).double())\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.mask * self.weight, self.bias)\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        mask = torch.from_numpy(mask.astype(np.uint8).T)\n",
    "        assert(self.mask.data.size() == mask.size())\n",
    "        self.mask.data.copy_(mask)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            init.zeros_(self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3, 4, 5, 6])]\n",
      "min m[-1] 1\n",
      "[1 2 3 4 5 6]\n",
      "1\n",
      "[5 2 1 4 5 5]\n",
      "1\n",
      "[2 3 3 2 5 3]\n",
      "2\n",
      "---------------------\n",
      "---------------------\n",
      "(6, 6)\n",
      "[1. 1. 1. 1. 1. 0.]\n",
      "[1. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 0.]\n",
      "[1. 1. 1. 1. 1. 0.]\n",
      "---------------------\n",
      "---------------------\n",
      "(6, 6)\n",
      "[0. 1. 1. 0. 0. 0.]\n",
      "[0. 1. 1. 0. 0. 0.]\n",
      "[0. 1. 1. 0. 0. 0.]\n",
      "[0. 1. 1. 0. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[0. 1. 1. 0. 0. 0.]\n",
      "---------------------\n",
      "---------------------\n",
      "(6, 6)\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 0. 1.]\n",
      "[1. 0. 0. 1. 0. 0.]\n",
      "[1. 0. 0. 1. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[1. 0. 0. 1. 0. 0.]\n",
      "---------------------\n",
      "---------------------\n",
      "(6, 6)\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 1. 0. 1.]\n",
      "[0. 0. 1. 1. 0. 1.]\n",
      "[0. 1. 1. 1. 0. 1.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "d = 6\n",
    "n_classes = 1\n",
    "input_size = d * n_classes\n",
    "hidden_size = 6\n",
    "n_hidden_layers = 3\n",
    "\n",
    "layers = nn.ModuleList()\n",
    "layers.append(MaskedLinear(input_size, hidden_size))\n",
    "for i in range(n_hidden_layers - 1):\n",
    "    layers.append(MaskedLinear(hidden_size, hidden_size))\n",
    "output_layer = MaskedLinear(hidden_size, input_size)\n",
    "\n",
    "# sampling m^1 vectors\n",
    "# if n_classes = 3, dim = 2\n",
    "# [1, 1, 1, 2, 2, 2]\n",
    "m = []\n",
    "m_0 = []\n",
    "for i in range(d):\n",
    "    m_0.extend([i + 1] * n_classes)\n",
    "\n",
    "m.append(np.array(m_0))\n",
    "\n",
    "print(m)\n",
    "\n",
    "print(\"min m[-1] {}\".format(min(m[-1])))\n",
    "\n",
    "for i in range(n_hidden_layers):\n",
    "    print(m[-1])\n",
    "    print(min(m[-1]))\n",
    "    m.append(np.random.randint(min(m[-1]), d, [hidden_size]))\n",
    "\n",
    "print('---------------------')\n",
    "    \n",
    "# Construct masks\n",
    "for i in np.arange(1, n_hidden_layers + 1):\n",
    "    # these are V and W, check V smaller current dim\n",
    "    # check W larger then current dim.\n",
    "    # \n",
    "    a = m[i].reshape(-1, 1)\n",
    "    b = m[i - 1].reshape(1, -1)\n",
    "    mask = (a >= b).astype(np.float)\n",
    "    print('---------------------')\n",
    "    print(mask.shape)\n",
    "    for row in mask:\n",
    "        print(row)\n",
    "    print('---------------------')\n",
    "    \n",
    "    layers[i - 1].set_mask(torch.from_numpy(mask))\n",
    "    \n",
    "a = m[0].reshape(-1, 1)\n",
    "b = m[i].reshape(1, -1)\n",
    "output_mask = (a > b).astype(np.float)\n",
    "output_layer.set_mask(torch.from_numpy(output_mask))\n",
    "\n",
    "print('---------------------')\n",
    "print(output_mask.shape)\n",
    "for row in output_mask:\n",
    "    print(row)\n",
    "print('---------------------')\n",
    "\n",
    "\n",
    "for i in range(1, len(layers)):\n",
    "    if i == 1:\n",
    "        tmp = np.dot(layers[i - 1].mask.data.numpy().T, layers[i].mask.data.numpy().T)\n",
    "    else:\n",
    "        tmp = np.dot(tmp, layers[i].mask.data.numpy().T)\n",
    "\n",
    "tmp = np.dot(tmp, output_mask.T)\n",
    "\n",
    "# print('---------------------')\n",
    "# print(test.shape)\n",
    "# for row in test.T:\n",
    "#     print(row)\n",
    "# print('---------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]),\n",
       " array([2, 2, 2, 2, 1, 2]),\n",
       " array([2, 2, 1, 1, 2, 2]),\n",
       " array([1, 1, 1, 1, 2, 1])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made mask unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1 2 2 1 2 2]\n",
      "[1 2 2 1 2 2]\n",
      "[1 2 2 1 2 2]\n",
      "[1, 2, 3]\n",
      "[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "d = 3\n",
    "n_classes = 1\n",
    "input_size = d * n_classes\n",
    "hidden_size = 6\n",
    "n_hidden_layers = 3\n",
    "\n",
    "# Generate m(k)\n",
    "m = []\n",
    "m_0 = list(range(1, (d * n_classes) + 1))\n",
    "m_end = list(range(1, (d * n_classes) + 1))\n",
    "m.append(m_0)\n",
    "for i in range(n_hidden_layers):\n",
    "    np.random.seed(0)\n",
    "    m_i = np.random.randint(min(m[-1]), d, hidden_size)\n",
    "    m.append(m_i)\n",
    "\n",
    "# output m\n",
    "m.append(m_end)\n",
    "\n",
    "for row in m:\n",
    "    print(row)\n",
    "    \n",
    "# Set the W mask values\n",
    "W_masks = []\n",
    "for i in range(1, n_hidden_layers + 1):\n",
    "    m_prev = np.asarray(m[i - 1])\n",
    "    m_curr = np.asarray(m[i])\n",
    "    \n",
    "    W = (m_prev.reshape(-1, 1) <= m_curr.reshape(1, -1)).astype(np.float)\n",
    "    W_masks.append(W)\n",
    "    \n",
    "# Set the V^M mask\n",
    "m_prev = np.asarray(m[-2])\n",
    "m_curr = np.asarray(m[-1])\n",
    "V = (m_prev.reshape(-1, 1) > m_curr.reshape(1, -1)).astype(np.float)\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3],\n",
       " array([1, 2, 2, 1, 2, 2]),\n",
       " array([2, 2, 2, 2, 2, 1]),\n",
       " array([1, 2, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 2 2 1 2 2]\n",
      "[1 2 2 1 2 2]\n",
      "[1 2 2 1 2 2]\n",
      "---------------------\n",
      "(3, 6)\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 1. 0. 0.]\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "m = []\n",
    "m_0 = []\n",
    "for i in range(d):\n",
    "    m_0.extend([i + 1] * n_classes)\n",
    "\n",
    "m.append(np.array(m_0))\n",
    "\n",
    "# print(m)\n",
    "\n",
    "# print(\"min m[-1] {}\".format(min(m[-1])))\n",
    "\n",
    "for i in range(n_hidden_layers):\n",
    "#     print(m[-1])\n",
    "#     print(min(m[-1]))\n",
    "    np.random.seed(0)\n",
    "    m.append(np.random.randint(min(m[-1]), d, [hidden_size]))\n",
    "\n",
    "for row in m:\n",
    "    print(row)\n",
    "    \n",
    "a = m[0].reshape(-1, 1)\n",
    "b = m[i].reshape(1, -1)\n",
    "output_mask = (a > b).astype(np.float)\n",
    "# output_layer.set_mask(torch.from_numpy(output_mask))\n",
    "\n",
    "print('---------------------')\n",
    "print(output_mask.shape)\n",
    "for row in output_mask:\n",
    "    print(row)\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADE MASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        self.register_buffer('mask', torch.ones([out_features, in_features]).double())\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.mask * self.weight, self.bias)\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        mask = torch.from_numpy(mask.astype(np.uint8).T)\n",
    "        assert(self.mask.data.size() == mask.size())\n",
    "        self.mask.data.copy_(mask)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            init.zeros_(self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "n_classes = 200\n",
    "input_size = d * n_classes\n",
    "hidden_sizes = [400, 800, 400]\n",
    "shuffle_input = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def made_test(input_size, hidden_sizes, shuffle_input=False):\n",
    "    hs = [input_size] + hidden_sizes + [input_size]\n",
    "    network = []\n",
    "    for in_feat, out_feat in zip(hs, hs[1:]):\n",
    "        network.extend([MaskedLinear(in_feat, out_feat),\n",
    "                       nn.ReLU(),\n",
    "                        ])\n",
    "    # remove last relu\n",
    "    network.pop()\n",
    "    network = nn.Sequential(*network)\n",
    "\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    # Build the masks\n",
    "    L = len(hidden_sizes)\n",
    "    m = {}\n",
    "    m[-1] = rng.permutation(input_size) if shuffle_input else np.arange(input_size)\n",
    "\n",
    "    for l in range(L):\n",
    "        m[l] = rng.randint(min(m[l - 1]), input_size - 1, size=hidden_sizes[l])\n",
    "\n",
    "    # Build W Masks\n",
    "    masks = []\n",
    "    for l in range(L):\n",
    "        W = m[l-1][:, None] <= m[l][None, :]\n",
    "        masks.append(W)\n",
    "    # V mask\n",
    "    V = m[L-1][:, None] < m[-1][None, :]\n",
    "    masks.append(V)\n",
    "\n",
    "    # Set masks in network\n",
    "    mask_layers = [l for l in network.modules() if isinstance(l, MaskedLinear)]\n",
    "    for layer, mask in zip(mask_layers, masks):\n",
    "        layer.set_mask(mask)\n",
    "#         sns.heatmap(mask.T)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = [(400, [400, 800, 400], True), \n",
    "          (400, [400, 800, 400], False),\n",
    "          (400, [400, 200, 400], False),\n",
    "          (400, [400, 400, 400], False),\n",
    "         ]\n",
    "\n",
    "config = [(10, [100, 120, 100], True), \n",
    "          (10, [100, 120, 100], False),\n",
    "         ]\n",
    "\n",
    "for (input_size, hidden_sizes, shuffle_input) in config:\n",
    "    made_test(input_size, hidden_sizes, shuffle_input=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 400\n",
      "400 800\n",
      "800 400\n",
      "400 400\n"
     ]
    }
   ],
   "source": [
    "for h0, h1 in zip(hs, hs[1:]):\n",
    "    print(\"{} {}\".format(h0, h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PixelCNN tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_in = 3\n",
    "H_out = 3\n",
    "FH, FW = 5, 5\n",
    "W = np.ones((H_out, H_in, FH, FW))\n",
    "mask = np.ones_like(W).astype('f')\n",
    "\n",
    "yc, xc = FH // 2, FW // 2\n",
    "offs = 1\n",
    "\n",
    "#type B, center = 1\n",
    "mask[:, :, yc + offs:, :] = 0.0\n",
    "mask[:, :, yc:, xc + offs:] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(mask_size, mask_type):\n",
    "    m = torch.zeros([mask_size, mask_size])\n",
    "    m[:mask_size // 2, :] = 1.0\n",
    "    m[mask_size // 2, :mask_size // 2] = 1.0\n",
    "    if mask_type == 'A':\n",
    "        m[mask_size // 2, mask_size // 2] = 1.0\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "mask = make_mask(5, 'B')\n",
    "print(mask.size())\n",
    "for row in mask:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        self.mask_type = mask_type\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kernel_height, kernel_width = self.weight.size()\n",
    "        self.mask.fill_(0)\n",
    "        half_h, half_w = kernel_height // 2, kernel_width // 2\n",
    "    \n",
    "        self.mask[:, :, :half_h, :] = 1.0\n",
    "        self.mask[:, :, half_h, :half_w] = 1.0\n",
    "        if self.mask_type == 'A':\n",
    "            self.mask[:, :, half_h, half_w] = 0.0\n",
    "        else:\n",
    "            self.mask[:, :, half_h, half_w] = 1.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.h = h\n",
    "        # todo: set padding to same\n",
    "        self.network = []\n",
    "        self.network.extend([\n",
    "            nn.Conv2d(self.h, self.h // 2, (1, 1)),\n",
    "            nn.BatchNorm2d(self.h // 2),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        self.network.extend([\n",
    "            MaskedConv2d('B', self.h // 2, self.h // 2, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(self.h // 2),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        \n",
    "        self.network.extend([\n",
    "            nn.Conv2d(self.h // 2, self.h, (1, 1)),\n",
    "            nn.BatchNorm2d(self.h),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        \n",
    "        self.network = nn.Sequential(*self.network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        x = self.network(x)\n",
    "        return F.relu(x + skip)\n",
    "\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.network = []\n",
    "        \n",
    "        # 7x7 Conv input, type A\n",
    "        self.network.extend([\n",
    "            MaskedConv2d('A', 3, self.in_channels, (7, 7), padding=3),\n",
    "            nn.BatchNorm2d(self.in_channels),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "\n",
    "        self.network.extend(\n",
    "            [ResidualBlock(self.in_channels) for _ in range(15)]\n",
    "        )\n",
    "        \n",
    "        # 3x3 Conv input, type B\n",
    "        self.network.extend([\n",
    "            MaskedConv2d('B', self.in_channels, self.in_channels, (3, 3), padding=1),\n",
    "            nn.BatchNorm2d(self.in_channels),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "        \n",
    "        # 1x1 Conv input\n",
    "        self.network.extend([\n",
    "            nn.Conv2d(self.in_channels, self.in_channels, (1, 1)),\n",
    "            nn.BatchNorm2d(self.in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, (1, 1)),\n",
    "        ])\n",
    "        \n",
    "        self.network = nn.Sequential(*self.network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        torch.reshape(x, (-1, 28, 28, 12))\n",
    "        sm = F.softmax(x, dim=-1)\n",
    "        return x, sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MaskedConv2d('A', 16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "input_tensor = torch.randn(20, 16, 50, 100)\n",
    "output = m(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128, 28, 28])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 28, 28])\n",
      "torch.Size([128, 64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "h = 128\n",
    "# net = nn.Conv2d(h, h // 2, (1, 1), padding= h // 4)\n",
    "net = nn.Conv2d(h, h // 2, (7, 7), padding=3)\n",
    "x = torch.Tensor(128, 128, 28, 28)\n",
    "print(x.size())\n",
    "out = net(x)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "h = 128\n",
    "\n",
    "network = []\n",
    "\n",
    "network.extend(\n",
    "    [ResidualBlock(h) for _ in range(15)]\n",
    ")\n",
    "\n",
    "network = nn.Sequential(*network)\n",
    "\n",
    "x = torch.Tensor(128, 128, 28, 28)\n",
    "out = network(x)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "h = 128\n",
    "\n",
    "network = []\n",
    "network.extend([\n",
    "    MaskedConv2d('A', 3, h, (7, 7), padding=3),\n",
    "    nn.BatchNorm2d(h),\n",
    "    nn.ReLU()\n",
    "])\n",
    "network.extend(\n",
    "    [ResidualBlock(h) for _ in range(15)]\n",
    ")\n",
    "\n",
    "# 3x3 Conv input, type B\n",
    "network.extend([\n",
    "    MaskedConv2d('B', h, h, (3, 3), padding=1),\n",
    "    nn.BatchNorm2d(h),\n",
    "    nn.ReLU(),\n",
    "])\n",
    "\n",
    "network.extend([\n",
    "    nn.Conv2d(h, h, (1, 1)),\n",
    "    nn.BatchNorm2d(h),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(h, 12, (1, 1)),\n",
    "])\n",
    "\n",
    "network = nn.Sequential(*network)\n",
    "\n",
    "x = torch.Tensor(128, 3, 28, 28)\n",
    "out = network(x)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN(128, 12)\n",
    "x = torch.Tensor(128, 3, 28, 28)\n",
    "out, sm = model(x)\n",
    "print(out.size())\n",
    "print(sm.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
